{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining with Titanic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitry/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import fcluster, single, complete\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform \n",
    "from collections.abc import Iterable\n",
    "from mlpipes.pfunc import get_ohe\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from mlpipes.pfunc import *\n",
    "from mlpipes.utils import iterate_over_pars\n",
    "import cachepy\n",
    "\n",
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "test = pd.read_csv(\"./data/test.csv\")\n",
    "combined = pd.concat([train, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = cachepy.Cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some preprocessing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_name(s): \n",
    "    a, b = s.split(',')\n",
    "    family_name = a.strip()\n",
    "    title = b.split('.')[0].strip()\n",
    "    first_name = b.split('.')[1].split()[0].strip()\n",
    "    return (first_name.replace('(', '').replace(')', ''), title, family_name)\n",
    "\n",
    "def parse_cabin_letter(column):\n",
    "    letter_pat = re.compile('([A-Za-z])\\d+')\n",
    "    return list(map(lambda x: letter_pat.findall(str(x))[0] if letter_pat.findall(str(x)) else pd.np.nan, column.values.tolist()))\n",
    "\n",
    "def parse_ticket_number(column):\n",
    "    number_pat = re.compile('\\d{3,}')\n",
    "    numbers = map(lambda x: number_pat.findall(x)[0] if number_pat.findall(x) else pd.np.nan, column)\n",
    "    return pd.Series(numbers)\n",
    "\n",
    "def get_friendship_group(df):\n",
    "    friendship_group_counter = 0\n",
    "    if 'family_name' not in df.columns:\n",
    "        family_names = pd.Series(map(lambda x: parse_name(x)[-1], df.Name))\n",
    "    else:\n",
    "        family_names = df.family_name\n",
    "    cabins = pd.Series(map(parse_cabin_letter, df.Cabin))\n",
    "    ticket_grouping = []\n",
    "    for family, count in family_names.value_counts().items():\n",
    "        family_mask = family_names == family\n",
    "        \n",
    "        if count == 1:\n",
    "            ticket_grouping.append(friendship_group_counter)\n",
    "            friendship_group_counter += 1\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def get_family_name(df):\n",
    "    family_names = pd.Series(map(lambda x: parse_name(x)[-1], df.Name))\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:,'family_name'] = family_names\n",
    "    return df_\n",
    "\n",
    "@cache\n",
    "def get_ticket_group(df):\n",
    "    df_ = df.copy()\n",
    "    grouped = df.Ticket.groupby(parse_ticket_number(df.Ticket))\n",
    "    for ind, key in enumerate(grouped.indices):\n",
    "        df_.loc[grouped.indices[key], 'ticket_group'] = ind if len(grouped.indices[key])>1 else -1\n",
    "    return df_\n",
    "\n",
    "@cache\n",
    "def get_cabin_letter(df):\n",
    "    df_ = df.copy()\n",
    "    cabins = parse_cabin_letter(df.Cabin)\n",
    "    df_.loc[:, 'cabin_na'] = pd.isnull(df.Cabin)\n",
    "    df_.loc[:, 'cabin'] = cabins\n",
    "    return df_\n",
    "\n",
    "@cache\n",
    "def get_is_alone(df):\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, 'is_alone'] = (df.loc[:, 'Parch'] + df.loc[:, 'SibSp'] + 1 == 1).astype(int)\n",
    "    return df_\n",
    "\n",
    "@cache\n",
    "def get_family_size(df):\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, 'family_size'] = df_.loc[:, 'Parch'] + df_.loc[:, 'SibSp'] + 1\n",
    "    return df_\n",
    "\n",
    "@cache\n",
    "def get_titles(df):\n",
    "    df_ = df.copy()\n",
    "    titles = pd.Series(map(lambda x: parse_name(x)[1], df.Name))\n",
    "    df_.loc[:, 'title'] = titles\n",
    "    return df_\n",
    "\n",
    "@cache\n",
    "def discretize_faries(df, ngroups=3):\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, 'fares'] = pd.cut(df_.loc[:,'Fare'], ngroups, labels=False)\n",
    "    return df_\n",
    "\n",
    "@cache\n",
    "def discretize_ages(df, ngroups=3):\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, 'Age'] = pd.cut(df_.loc[:,'Age'], ngroups, labels=False)\n",
    "    return df_\n",
    "\n",
    "@cache\n",
    "def estimate_age(df):\n",
    "    estimates = []\n",
    "    for ind, row in df.loc[df.Age.isnull(), :].iterrows():\n",
    "        # NOTE: Could be rewritten using vectorized notation\n",
    "        if row.title in ['Master', 'Mr', 'Miss', 'Rev', 'Dr']:\n",
    "            estimates.append(df.groupby(['title', 'Sex']).median().loc[[row.title, row.Sex],'Age'].values[0])\n",
    "        else:\n",
    "            estimates.append(df.groupby(['Sex', 'Pclass']).median().loc[[row.Sex], 'Age'].values[0])\n",
    "    df_ = df.copy()\n",
    "    df_.loc[df.Age.isnull(), 'Age'] = estimates\n",
    "    return df_\n",
    "\n",
    "@cache\n",
    "def get_cabin_groups(df):\n",
    "    num_pat = re.compile('\\d+')\n",
    "    let_pat = re.compile('[a-zA-Z]')\n",
    "    LONG_DISTANCE = 5\n",
    "    MEDIUM_DISTANCE = 4\n",
    "    NORMAL_DISTANCE = 3\n",
    "    SMALL_DISTANCE = 2\n",
    "    LOW_DISTANCE = 1\n",
    "    EQUAL = 0\n",
    "    def cabin_distance(u, v):\n",
    "        _u, _v = u[0], v[0]\n",
    "        if not isinstance(_u, Iterable) or not isinstance(_v, Iterable):\n",
    "            return LONG_DISTANCE\n",
    "        unums = list(map(int, sum(map(num_pat.findall, _u), [])))\n",
    "        vnums = list(map(int, sum(map(num_pat.findall, _v), [])))\n",
    "        ulets = list(sum(map(let_pat.findall, _u), []))\n",
    "        vlets = list(sum(map(let_pat.findall, _v), []))\n",
    "        if not(unums and vnums):\n",
    "            if set(ulets).intersection(vlets):\n",
    "                return EQUAL\n",
    "            else:\n",
    "                return MEDIUM_DISTANCE\n",
    "        if u == v:\n",
    "            return EQUAL\n",
    "        if set(_u).intersection(set(_v)):\n",
    "            return LOW_DISTANCE\n",
    "        if not set(ulets).intersection(set(vlets)):\n",
    "            return MEDIUM_DISTANCE\n",
    "        else:\n",
    "            for p in _u:\n",
    "                for q in _v:\n",
    "                    try:\n",
    "                        pval = list(map(int, num_pat.findall(p)))[0]\n",
    "                        qval = list(map(int, num_pat.findall(q)))[0]\n",
    "                        if p[0] == q[0] and (abs(pval - qval) <= 2):\n",
    "                            return SMALL_DISTANCE\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "            return NORMAL_DISTANCE\n",
    "        return MEDIUM_DISTANCE\n",
    "    distances = pdist(df.Cabin.apply(lambda x: x.split() if not isinstance(x, float) else x).values[:, np.newaxis], cabin_distance)\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, 'cabin_group'] = fcluster(complete(distances), SMALL_DISTANCE, criterion='distance')\n",
    "    df_.cabin_group = df_.groupby('cabin_group')['cabin_group'].transform(lambda x: x if len(x)>1 else pd.Series([-1]*len(x)))\n",
    "    return df_\n",
    "\n",
    "@cache\n",
    "def combine_titles(df):\n",
    "    df_ = df.copy()\n",
    "    df_['title'] = df_['title'].replace(['Mlle'], 'Miss')\n",
    "    df_['title'] = df_['title'].replace(['Ms'], 'Miss')\n",
    "    df_['title'] = df_['title'].replace(['Mme'], 'Mrs')\n",
    "    df_['title'] = df_['title'].replace(['Lady', 'the Countess', 'Capt', 'Col',\n",
    "                                         'Don', 'Dr', 'Major', 'Rev', 'Sir',\n",
    "                                         'Jonkheer', 'Dona'], 'rare')\n",
    "    return df_\n",
    "\n",
    "@cache\n",
    "def simple_encoder(df):\n",
    "    df_ = df.copy()\n",
    "    sex_mapping = {'male': 0, 'female':1}\n",
    "    embarked_mapping = {'S':0, 'Q':1, 'S':2}\n",
    "    df_.Embarked = df_.Embarked.map(embarked_mapping)\n",
    "    df_.Sex = df_.Sex.map(sex_mapping)\n",
    "    return df_\n",
    "\n",
    "@cache\n",
    "def label_encode(df, **kwargs):\n",
    "    return get_le(df, **kwargs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = (('add_ticket_group', get_ticket_group, {}),\n",
    "                          ('add_family_size', get_family_size, {}),\n",
    "                          ('add_titles', get_titles, {}),\n",
    "                          ('convert_fares', discretize_faries, {'ngroups': 3}),\n",
    "                          ('fill_embarked', fill_na_simple, {'colnames': ('Embarked',),\n",
    "                                                             'methods': (lambda x: pd.Series(x).mode()[0],)}),\n",
    "                          ('add_family_name', get_family_name, {}),\n",
    "                          ('add_ages', estimate_age, {}),\n",
    "                          ('convert_ages', discretize_ages, {'ngroups': 5}),\n",
    "                          ('add_cabin_groups', get_cabin_groups, {}),\n",
    "                          ('combine_titles', combine_titles, {}),\n",
    "                          ('sex_encoder', simple_encoder, {}),\n",
    "                          ('drop_columns', drop_columns, {'colnames': ('Survived',\n",
    "                                                                       'PassengerId',\n",
    "                                                                       'SibSp',\n",
    "                                                                       'Parch',\n",
    "                                                                       'Ticket',\n",
    "                                                                       'Fare',\n",
    "                                                                       'Name',\n",
    "                                                                       'Cabin',\n",
    "                                                                       )}),\n",
    "                          ('get_le', label_encode, {'colnames': ('ticket_group', 'cabin_group',\n",
    "                                                                 'title', 'Embarked', 'family_name')})\n",
    "                         )\n",
    "\n",
    "def process(pipeline, data, parameters=dict()):\n",
    "    data_ = data.copy()\n",
    "    for name, func, kwargs in pipeline:\n",
    "        if name in parameters:\n",
    "            kwargs = parameters[name]\n",
    "        data_ = func(data_, **kwargs)\n",
    "    return data_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing steps (feature engeneering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed  = process(preprocessing_pipeline, combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>family_size</th>\n",
       "      <th>fares</th>\n",
       "      <th>LE_ticket_group</th>\n",
       "      <th>LE_cabin_group</th>\n",
       "      <th>LE_title</th>\n",
       "      <th>LE_Embarked</th>\n",
       "      <th>LE_family_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Pclass  Sex  family_size  fares  LE_ticket_group  LE_cabin_group  \\\n",
       "0    1       3    0            2    0.0                0               0   \n",
       "1    2       1    1            2    0.0               63               9   \n",
       "2    1       3    1            1    0.0                0               0   \n",
       "3    2       1    1            2    0.0               20              21   \n",
       "4    2       3    0            1    0.0                0               0   \n",
       "\n",
       "   LE_title  LE_Embarked  LE_family_name  \n",
       "0         2            1             100  \n",
       "1         3            6             182  \n",
       "2         1            1             329  \n",
       "3         3            1             267  \n",
       "4         2            1              15  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_GB = {'n_estimators': randint(20, 300),\n",
    "                 'max_depth': randint(3, 20),\n",
    "                 'subsample': uniform(0.7, 0.3),\n",
    "                 'learning_rate': uniform(0.0001, 0.3),\n",
    "                 'max_features': ('auto', 'log2', None),\n",
    "                 'min_samples_leaf': randint(3, 10),\n",
    "                 'min_samples_split' : randint(2, 10)\n",
    "                }\n",
    "preprocessing_parameters = {'convert_ages': {'ngroups': [2,3,4,5]},\n",
    "                            'convert_fares': {'ngroups': [2,3,4,5]}\n",
    "                           }\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "y = train.Survived.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Processing par:  {'convert_ages': {'ngroups': 2}, 'convert_fares': {'ngroups': 2}}\n",
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 358 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 608 tasks      | elapsed:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done 958 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1408 tasks      | elapsed:   48.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1958 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2608 tasks      | elapsed:  1.5min\n"
     ]
    }
   ],
   "source": [
    "for par in iterate_over_pars(preprocessing_parameters):\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Processing par: \", par)\n",
    "    processed  = process(preprocessing_pipeline, combined, parameters=par)\n",
    "    X = processed.iloc[:train.shape[0]].values\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        clfcv = RandomizedSearchCV(clf, param_distributions=parameters_GB, n_iter=1000, scoring='f1',\n",
    "                               cv=3, verbose=1, n_jobs=-1)\n",
    "        clfcv.fit(X, y)\n",
    "    \n",
    "    cross_val_score(clfcv.best_estimator_, X, y, cv=5, scoring='f1')\n",
    "    clfcv.best_score_\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-b7f487b2d22b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "\n",
    "BATCH_SIZE = 15\n",
    "NUM_CLASSES = 2\n",
    "INPUT_SHAPE = processed.shape[1]\n",
    "EPOCHS = 2000\n",
    "\n",
    "# ------------- Building the model ----------------\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(INPUT_SHAPE,), activation='relu'))\n",
    "model.add(Dense(100, activation='elu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(30, activation='elu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10, activation='elu'))\n",
    "model.add(Dense(NUM_CLASSES-1, activation='sigmoid'))\n",
    "# -------------------------------------------------\n",
    "\n",
    "\n",
    "# ---- Building the training and test data --------\n",
    "X = processed.iloc[:train.shape[0]].values\n",
    "Y = train.Survived.values\n",
    "#x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42, stratify=Y)\n",
    "\n",
    "#y_train = keras.utils.to_categorical(y_train, 3)\n",
    "#y_test = keras.utils.to_categorical(y_test, 3)\n",
    "# -------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 20)                200       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 100)               2100      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 9,721\n",
      "Trainable params: 9,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/2000\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 0.8193 - acc: 0.6132 - val_loss: 0.6260 - val_acc: 0.6679\n",
      "Epoch 2/2000\n",
      "623/623 [==============================] - 0s 245us/step - loss: 0.6879 - acc: 0.5987 - val_loss: 0.5864 - val_acc: 0.6716\n",
      "Epoch 3/2000\n",
      "623/623 [==============================] - 0s 293us/step - loss: 0.6329 - acc: 0.6453 - val_loss: 0.5571 - val_acc: 0.7351\n",
      "Epoch 4/2000\n",
      "623/623 [==============================] - 0s 204us/step - loss: 0.6085 - acc: 0.6404 - val_loss: 0.5699 - val_acc: 0.6828\n",
      "Epoch 5/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.6225 - acc: 0.6180 - val_loss: 0.5484 - val_acc: 0.6978\n",
      "Epoch 6/2000\n",
      "623/623 [==============================] - 0s 345us/step - loss: 0.5864 - acc: 0.6742 - val_loss: 0.5451 - val_acc: 0.7164\n",
      "Epoch 7/2000\n",
      "623/623 [==============================] - 0s 260us/step - loss: 0.5987 - acc: 0.6629 - val_loss: 0.5452 - val_acc: 0.6791\n",
      "Epoch 8/2000\n",
      "623/623 [==============================] - 0s 278us/step - loss: 0.6018 - acc: 0.6501 - val_loss: 0.5453 - val_acc: 0.6866\n",
      "Epoch 9/2000\n",
      "623/623 [==============================] - 0s 275us/step - loss: 0.5864 - acc: 0.6806 - val_loss: 0.5430 - val_acc: 0.7164\n",
      "Epoch 10/2000\n",
      "623/623 [==============================] - 0s 288us/step - loss: 0.5947 - acc: 0.6292 - val_loss: 0.5355 - val_acc: 0.7127\n",
      "Epoch 11/2000\n",
      "623/623 [==============================] - 0s 260us/step - loss: 0.5894 - acc: 0.6581 - val_loss: 0.5326 - val_acc: 0.7276\n",
      "Epoch 12/2000\n",
      "623/623 [==============================] - 0s 250us/step - loss: 0.5761 - acc: 0.6758 - val_loss: 0.5361 - val_acc: 0.7090\n",
      "Epoch 13/2000\n",
      "623/623 [==============================] - 0s 259us/step - loss: 0.5937 - acc: 0.6822 - val_loss: 0.5420 - val_acc: 0.6940\n",
      "Epoch 14/2000\n",
      "623/623 [==============================] - 0s 259us/step - loss: 0.5864 - acc: 0.6645 - val_loss: 0.5369 - val_acc: 0.7127\n",
      "Epoch 15/2000\n",
      "623/623 [==============================] - 0s 287us/step - loss: 0.5850 - acc: 0.6918 - val_loss: 0.5363 - val_acc: 0.6940\n",
      "Epoch 16/2000\n",
      "623/623 [==============================] - 0s 281us/step - loss: 0.5849 - acc: 0.6934 - val_loss: 0.5383 - val_acc: 0.6828\n",
      "Epoch 17/2000\n",
      "623/623 [==============================] - 0s 276us/step - loss: 0.5703 - acc: 0.6934 - val_loss: 0.5405 - val_acc: 0.6828\n",
      "Epoch 18/2000\n",
      "623/623 [==============================] - 0s 272us/step - loss: 0.5714 - acc: 0.6790 - val_loss: 0.5388 - val_acc: 0.6866\n",
      "Epoch 19/2000\n",
      "623/623 [==============================] - 0s 267us/step - loss: 0.5702 - acc: 0.6918 - val_loss: 0.5348 - val_acc: 0.7164\n",
      "Epoch 20/2000\n",
      "623/623 [==============================] - 0s 271us/step - loss: 0.5795 - acc: 0.6806 - val_loss: 0.5397 - val_acc: 0.7090\n",
      "Epoch 21/2000\n",
      "623/623 [==============================] - 0s 309us/step - loss: 0.5767 - acc: 0.6693 - val_loss: 0.5429 - val_acc: 0.6903\n",
      "Epoch 22/2000\n",
      "623/623 [==============================] - 0s 263us/step - loss: 0.5722 - acc: 0.6774 - val_loss: 0.5347 - val_acc: 0.7052\n",
      "Epoch 23/2000\n",
      "623/623 [==============================] - 0s 286us/step - loss: 0.5688 - acc: 0.7030 - val_loss: 0.5322 - val_acc: 0.7127\n",
      "Epoch 24/2000\n",
      "623/623 [==============================] - 0s 291us/step - loss: 0.5641 - acc: 0.6950 - val_loss: 0.5335 - val_acc: 0.7015\n",
      "Epoch 25/2000\n",
      "623/623 [==============================] - 0s 341us/step - loss: 0.5703 - acc: 0.6726 - val_loss: 0.5316 - val_acc: 0.7164\n",
      "Epoch 26/2000\n",
      "623/623 [==============================] - 0s 278us/step - loss: 0.5544 - acc: 0.7095 - val_loss: 0.5367 - val_acc: 0.7052\n",
      "Epoch 27/2000\n",
      "623/623 [==============================] - 0s 286us/step - loss: 0.5723 - acc: 0.6677 - val_loss: 0.5377 - val_acc: 0.7239\n",
      "Epoch 28/2000\n",
      "623/623 [==============================] - 0s 261us/step - loss: 0.5650 - acc: 0.6693 - val_loss: 0.5468 - val_acc: 0.7015\n",
      "Epoch 29/2000\n",
      "623/623 [==============================] - 0s 263us/step - loss: 0.5756 - acc: 0.6822 - val_loss: 0.5469 - val_acc: 0.6940\n",
      "Epoch 30/2000\n",
      "623/623 [==============================] - 0s 254us/step - loss: 0.5620 - acc: 0.6918 - val_loss: 0.5443 - val_acc: 0.7276\n",
      "Epoch 31/2000\n",
      "623/623 [==============================] - 0s 212us/step - loss: 0.5750 - acc: 0.6581 - val_loss: 0.5432 - val_acc: 0.7313\n",
      "Epoch 32/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.5655 - acc: 0.6661 - val_loss: 0.5404 - val_acc: 0.7239\n",
      "Epoch 33/2000\n",
      "623/623 [==============================] - 0s 276us/step - loss: 0.5661 - acc: 0.6886 - val_loss: 0.5408 - val_acc: 0.7276\n",
      "Epoch 34/2000\n",
      "623/623 [==============================] - 0s 267us/step - loss: 0.5629 - acc: 0.6886 - val_loss: 0.5440 - val_acc: 0.7164\n",
      "Epoch 35/2000\n",
      "623/623 [==============================] - 0s 282us/step - loss: 0.5684 - acc: 0.6677 - val_loss: 0.5477 - val_acc: 0.7201\n",
      "Epoch 36/2000\n",
      "623/623 [==============================] - 0s 265us/step - loss: 0.5685 - acc: 0.6870 - val_loss: 0.5485 - val_acc: 0.7164\n",
      "Epoch 37/2000\n",
      "623/623 [==============================] - 0s 280us/step - loss: 0.5453 - acc: 0.6998 - val_loss: 0.5435 - val_acc: 0.7164\n",
      "Epoch 38/2000\n",
      "623/623 [==============================] - 0s 271us/step - loss: 0.5675 - acc: 0.6806 - val_loss: 0.5397 - val_acc: 0.7090\n",
      "Epoch 39/2000\n",
      "623/623 [==============================] - 0s 283us/step - loss: 0.5642 - acc: 0.7063 - val_loss: 0.5449 - val_acc: 0.7090\n",
      "Epoch 40/2000\n",
      "623/623 [==============================] - 0s 242us/step - loss: 0.5596 - acc: 0.6886 - val_loss: 0.5399 - val_acc: 0.7164\n",
      "Epoch 41/2000\n",
      "623/623 [==============================] - 0s 224us/step - loss: 0.5607 - acc: 0.6950 - val_loss: 0.5477 - val_acc: 0.7164\n",
      "Epoch 42/2000\n",
      "623/623 [==============================] - 0s 238us/step - loss: 0.5558 - acc: 0.7014 - val_loss: 0.5471 - val_acc: 0.7164\n",
      "Epoch 43/2000\n",
      "623/623 [==============================] - 0s 199us/step - loss: 0.5549 - acc: 0.7047 - val_loss: 0.5474 - val_acc: 0.6978\n",
      "Epoch 44/2000\n",
      "623/623 [==============================] - 0s 228us/step - loss: 0.5542 - acc: 0.7191 - val_loss: 0.5421 - val_acc: 0.7090\n",
      "Epoch 45/2000\n",
      "623/623 [==============================] - 0s 285us/step - loss: 0.5491 - acc: 0.6966 - val_loss: 0.5476 - val_acc: 0.7239\n",
      "Epoch 46/2000\n",
      "623/623 [==============================] - 0s 267us/step - loss: 0.5658 - acc: 0.6998 - val_loss: 0.5444 - val_acc: 0.7351\n",
      "Epoch 47/2000\n",
      "623/623 [==============================] - 0s 242us/step - loss: 0.5525 - acc: 0.6934 - val_loss: 0.5421 - val_acc: 0.7239\n",
      "Epoch 48/2000\n",
      "623/623 [==============================] - 0s 330us/step - loss: 0.5582 - acc: 0.6998 - val_loss: 0.5485 - val_acc: 0.7425\n",
      "Epoch 49/2000\n",
      "623/623 [==============================] - 0s 273us/step - loss: 0.5671 - acc: 0.6822 - val_loss: 0.5457 - val_acc: 0.7463\n",
      "Epoch 50/2000\n",
      "623/623 [==============================] - 0s 268us/step - loss: 0.5592 - acc: 0.6790 - val_loss: 0.5471 - val_acc: 0.7388\n",
      "Epoch 51/2000\n",
      "623/623 [==============================] - 0s 273us/step - loss: 0.5487 - acc: 0.7143 - val_loss: 0.5452 - val_acc: 0.7351\n",
      "Epoch 52/2000\n",
      "623/623 [==============================] - 0s 276us/step - loss: 0.5500 - acc: 0.7095 - val_loss: 0.5456 - val_acc: 0.7313\n",
      "Epoch 53/2000\n",
      "623/623 [==============================] - 0s 277us/step - loss: 0.5529 - acc: 0.7239 - val_loss: 0.5448 - val_acc: 0.7164\n",
      "Epoch 54/2000\n",
      "623/623 [==============================] - 0s 274us/step - loss: 0.5577 - acc: 0.6742 - val_loss: 0.5414 - val_acc: 0.7239\n",
      "Epoch 55/2000\n",
      "623/623 [==============================] - 0s 261us/step - loss: 0.5520 - acc: 0.6870 - val_loss: 0.5408 - val_acc: 0.7127\n",
      "Epoch 56/2000\n",
      "623/623 [==============================] - 0s 240us/step - loss: 0.5517 - acc: 0.7014 - val_loss: 0.5387 - val_acc: 0.7351\n",
      "Epoch 57/2000\n",
      "623/623 [==============================] - 0s 296us/step - loss: 0.5525 - acc: 0.6950 - val_loss: 0.5402 - val_acc: 0.7388\n",
      "Epoch 58/2000\n",
      "623/623 [==============================] - 0s 235us/step - loss: 0.5551 - acc: 0.7047 - val_loss: 0.5414 - val_acc: 0.7090\n",
      "Epoch 59/2000\n",
      "623/623 [==============================] - 0s 214us/step - loss: 0.5524 - acc: 0.7223 - val_loss: 0.5453 - val_acc: 0.7127\n",
      "Epoch 60/2000\n",
      "623/623 [==============================] - 0s 209us/step - loss: 0.5470 - acc: 0.6998 - val_loss: 0.5425 - val_acc: 0.7276\n",
      "Epoch 61/2000\n",
      "623/623 [==============================] - 0s 223us/step - loss: 0.5410 - acc: 0.7191 - val_loss: 0.5469 - val_acc: 0.7313\n",
      "Epoch 62/2000\n",
      "623/623 [==============================] - 0s 207us/step - loss: 0.5477 - acc: 0.6822 - val_loss: 0.5422 - val_acc: 0.7388\n",
      "Epoch 63/2000\n",
      "623/623 [==============================] - 0s 215us/step - loss: 0.5408 - acc: 0.7159 - val_loss: 0.5417 - val_acc: 0.7351\n",
      "Epoch 64/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.5582 - acc: 0.7014 - val_loss: 0.5399 - val_acc: 0.7313\n",
      "Epoch 65/2000\n",
      "623/623 [==============================] - 0s 219us/step - loss: 0.5540 - acc: 0.7127 - val_loss: 0.5433 - val_acc: 0.7388\n",
      "Epoch 66/2000\n",
      "623/623 [==============================] - 0s 222us/step - loss: 0.5407 - acc: 0.7143 - val_loss: 0.5402 - val_acc: 0.7351\n",
      "Epoch 67/2000\n",
      "623/623 [==============================] - 0s 218us/step - loss: 0.5517 - acc: 0.7111 - val_loss: 0.5456 - val_acc: 0.7351\n",
      "Epoch 68/2000\n",
      "623/623 [==============================] - 0s 222us/step - loss: 0.5373 - acc: 0.7287 - val_loss: 0.5424 - val_acc: 0.7351\n",
      "Epoch 69/2000\n",
      "623/623 [==============================] - 0s 237us/step - loss: 0.5483 - acc: 0.7127 - val_loss: 0.5387 - val_acc: 0.7500\n",
      "Epoch 70/2000\n",
      "623/623 [==============================] - 0s 261us/step - loss: 0.5491 - acc: 0.7111 - val_loss: 0.5369 - val_acc: 0.7351\n",
      "Epoch 71/2000\n",
      "623/623 [==============================] - 0s 257us/step - loss: 0.5452 - acc: 0.7079 - val_loss: 0.5356 - val_acc: 0.7425\n",
      "Epoch 72/2000\n",
      "623/623 [==============================] - 0s 249us/step - loss: 0.5484 - acc: 0.7207 - val_loss: 0.5346 - val_acc: 0.7388\n",
      "Epoch 73/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.5456 - acc: 0.7047 - val_loss: 0.5398 - val_acc: 0.7425\n",
      "Epoch 74/2000\n",
      "623/623 [==============================] - 0s 262us/step - loss: 0.5406 - acc: 0.7111 - val_loss: 0.5403 - val_acc: 0.7313\n",
      "Epoch 75/2000\n",
      "623/623 [==============================] - 0s 243us/step - loss: 0.5455 - acc: 0.7030 - val_loss: 0.5355 - val_acc: 0.7463\n",
      "Epoch 76/2000\n",
      "623/623 [==============================] - 0s 259us/step - loss: 0.5413 - acc: 0.7287 - val_loss: 0.5427 - val_acc: 0.7313\n",
      "Epoch 77/2000\n",
      "623/623 [==============================] - 0s 262us/step - loss: 0.5404 - acc: 0.7207 - val_loss: 0.5435 - val_acc: 0.7313\n",
      "Epoch 78/2000\n",
      "623/623 [==============================] - 0s 261us/step - loss: 0.5417 - acc: 0.7111 - val_loss: 0.5401 - val_acc: 0.7351\n",
      "Epoch 79/2000\n",
      "623/623 [==============================] - 0s 275us/step - loss: 0.5576 - acc: 0.6998 - val_loss: 0.5453 - val_acc: 0.7239\n",
      "Epoch 80/2000\n",
      "623/623 [==============================] - 0s 232us/step - loss: 0.5399 - acc: 0.7191 - val_loss: 0.5407 - val_acc: 0.7463\n",
      "Epoch 81/2000\n",
      "623/623 [==============================] - 0s 303us/step - loss: 0.5378 - acc: 0.7175 - val_loss: 0.5474 - val_acc: 0.7388\n",
      "Epoch 82/2000\n",
      "623/623 [==============================] - 0s 267us/step - loss: 0.5419 - acc: 0.7175 - val_loss: 0.5379 - val_acc: 0.7425\n",
      "Epoch 83/2000\n",
      "623/623 [==============================] - 0s 235us/step - loss: 0.5451 - acc: 0.7095 - val_loss: 0.5354 - val_acc: 0.7500\n",
      "Epoch 84/2000\n",
      "623/623 [==============================] - 0s 258us/step - loss: 0.5407 - acc: 0.7079 - val_loss: 0.5357 - val_acc: 0.7463\n",
      "Epoch 85/2000\n",
      "623/623 [==============================] - 0s 242us/step - loss: 0.5398 - acc: 0.7095 - val_loss: 0.5366 - val_acc: 0.7425\n",
      "Epoch 86/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.5346 - acc: 0.7175 - val_loss: 0.5388 - val_acc: 0.7425\n",
      "Epoch 87/2000\n",
      "623/623 [==============================] - 0s 239us/step - loss: 0.5460 - acc: 0.7063 - val_loss: 0.5371 - val_acc: 0.7537\n",
      "Epoch 88/2000\n",
      "623/623 [==============================] - 0s 222us/step - loss: 0.5322 - acc: 0.7352 - val_loss: 0.5364 - val_acc: 0.7575\n",
      "Epoch 89/2000\n",
      "623/623 [==============================] - 0s 217us/step - loss: 0.5391 - acc: 0.7127 - val_loss: 0.5369 - val_acc: 0.7500\n",
      "Epoch 90/2000\n",
      "623/623 [==============================] - 0s 220us/step - loss: 0.5426 - acc: 0.7111 - val_loss: 0.5367 - val_acc: 0.7500\n",
      "Epoch 91/2000\n",
      "623/623 [==============================] - 0s 260us/step - loss: 0.5320 - acc: 0.7014 - val_loss: 0.5402 - val_acc: 0.7575\n",
      "Epoch 92/2000\n",
      "623/623 [==============================] - 0s 209us/step - loss: 0.5362 - acc: 0.7255 - val_loss: 0.5399 - val_acc: 0.7612\n",
      "Epoch 93/2000\n",
      "623/623 [==============================] - 0s 245us/step - loss: 0.5498 - acc: 0.7175 - val_loss: 0.5414 - val_acc: 0.7500\n",
      "Epoch 94/2000\n",
      "623/623 [==============================] - 0s 243us/step - loss: 0.5342 - acc: 0.7159 - val_loss: 0.5431 - val_acc: 0.7500\n",
      "Epoch 95/2000\n",
      "623/623 [==============================] - 0s 260us/step - loss: 0.5182 - acc: 0.7400 - val_loss: 0.5387 - val_acc: 0.7500\n",
      "Epoch 96/2000\n",
      "623/623 [==============================] - 0s 220us/step - loss: 0.5440 - acc: 0.7014 - val_loss: 0.5396 - val_acc: 0.7575\n",
      "Epoch 97/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.5257 - acc: 0.7319 - val_loss: 0.5424 - val_acc: 0.7575\n",
      "Epoch 98/2000\n",
      "623/623 [==============================] - 0s 267us/step - loss: 0.5396 - acc: 0.7143 - val_loss: 0.5362 - val_acc: 0.7612\n",
      "Epoch 99/2000\n",
      "623/623 [==============================] - 0s 240us/step - loss: 0.5450 - acc: 0.7175 - val_loss: 0.5341 - val_acc: 0.7575\n",
      "Epoch 100/2000\n",
      "623/623 [==============================] - 0s 248us/step - loss: 0.5249 - acc: 0.7352 - val_loss: 0.5385 - val_acc: 0.7537\n",
      "Epoch 101/2000\n",
      "623/623 [==============================] - 0s 262us/step - loss: 0.5365 - acc: 0.7271 - val_loss: 0.5327 - val_acc: 0.7575\n",
      "Epoch 102/2000\n",
      "623/623 [==============================] - 0s 256us/step - loss: 0.5295 - acc: 0.7287 - val_loss: 0.5378 - val_acc: 0.7500\n",
      "Epoch 103/2000\n",
      "623/623 [==============================] - 0s 267us/step - loss: 0.5396 - acc: 0.7079 - val_loss: 0.5394 - val_acc: 0.7500\n",
      "Epoch 104/2000\n",
      "623/623 [==============================] - 0s 262us/step - loss: 0.5331 - acc: 0.7335 - val_loss: 0.5381 - val_acc: 0.7425\n",
      "Epoch 105/2000\n",
      "623/623 [==============================] - 0s 262us/step - loss: 0.5417 - acc: 0.7095 - val_loss: 0.5393 - val_acc: 0.7425\n",
      "Epoch 106/2000\n",
      "623/623 [==============================] - 0s 211us/step - loss: 0.5465 - acc: 0.7143 - val_loss: 0.5418 - val_acc: 0.7463\n",
      "Epoch 107/2000\n",
      "623/623 [==============================] - 0s 243us/step - loss: 0.5338 - acc: 0.7239 - val_loss: 0.5442 - val_acc: 0.7388\n",
      "Epoch 108/2000\n",
      "623/623 [==============================] - 0s 248us/step - loss: 0.5346 - acc: 0.7287 - val_loss: 0.5460 - val_acc: 0.7388\n",
      "Epoch 109/2000\n",
      "623/623 [==============================] - 0s 222us/step - loss: 0.5402 - acc: 0.7223 - val_loss: 0.5438 - val_acc: 0.7388\n",
      "Epoch 110/2000\n",
      "623/623 [==============================] - 0s 275us/step - loss: 0.5323 - acc: 0.7255 - val_loss: 0.5489 - val_acc: 0.7425\n",
      "Epoch 111/2000\n",
      "623/623 [==============================] - 0s 259us/step - loss: 0.5227 - acc: 0.7239 - val_loss: 0.5503 - val_acc: 0.7425\n",
      "Epoch 112/2000\n",
      "623/623 [==============================] - 0s 229us/step - loss: 0.5357 - acc: 0.7271 - val_loss: 0.5460 - val_acc: 0.7425\n",
      "Epoch 113/2000\n",
      "623/623 [==============================] - 0s 265us/step - loss: 0.5289 - acc: 0.7335 - val_loss: 0.5483 - val_acc: 0.7425\n",
      "Epoch 114/2000\n",
      "623/623 [==============================] - 0s 232us/step - loss: 0.5157 - acc: 0.7448 - val_loss: 0.5447 - val_acc: 0.7425\n",
      "Epoch 115/2000\n",
      "623/623 [==============================] - 0s 250us/step - loss: 0.5366 - acc: 0.7271 - val_loss: 0.5475 - val_acc: 0.7425\n",
      "Epoch 116/2000\n",
      "623/623 [==============================] - 0s 271us/step - loss: 0.5397 - acc: 0.7271 - val_loss: 0.5410 - val_acc: 0.7425\n",
      "Epoch 117/2000\n",
      "623/623 [==============================] - 0s 270us/step - loss: 0.5197 - acc: 0.7255 - val_loss: 0.5397 - val_acc: 0.7388\n",
      "Epoch 118/2000\n",
      "623/623 [==============================] - 0s 225us/step - loss: 0.5277 - acc: 0.7319 - val_loss: 0.5405 - val_acc: 0.7351\n",
      "Epoch 119/2000\n",
      "623/623 [==============================] - 0s 223us/step - loss: 0.5441 - acc: 0.7207 - val_loss: 0.5471 - val_acc: 0.7425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/2000\n",
      "623/623 [==============================] - 0s 229us/step - loss: 0.5399 - acc: 0.7207 - val_loss: 0.5418 - val_acc: 0.7388\n",
      "Epoch 121/2000\n",
      "623/623 [==============================] - 0s 209us/step - loss: 0.5228 - acc: 0.7255 - val_loss: 0.5443 - val_acc: 0.7388\n",
      "Epoch 122/2000\n",
      "623/623 [==============================] - 0s 260us/step - loss: 0.5209 - acc: 0.7480 - val_loss: 0.5414 - val_acc: 0.7425\n",
      "Epoch 123/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.5201 - acc: 0.7287 - val_loss: 0.5392 - val_acc: 0.7388\n",
      "Epoch 124/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.5198 - acc: 0.7287 - val_loss: 0.5421 - val_acc: 0.7388\n",
      "Epoch 125/2000\n",
      "623/623 [==============================] - 0s 243us/step - loss: 0.5301 - acc: 0.7319 - val_loss: 0.5402 - val_acc: 0.7500\n",
      "Epoch 126/2000\n",
      "623/623 [==============================] - 0s 257us/step - loss: 0.5247 - acc: 0.7400 - val_loss: 0.5392 - val_acc: 0.7500\n",
      "Epoch 127/2000\n",
      "623/623 [==============================] - 0s 258us/step - loss: 0.5294 - acc: 0.7175 - val_loss: 0.5363 - val_acc: 0.7537\n",
      "Epoch 128/2000\n",
      "623/623 [==============================] - 0s 236us/step - loss: 0.5251 - acc: 0.7464 - val_loss: 0.5427 - val_acc: 0.7425\n",
      "Epoch 129/2000\n",
      "623/623 [==============================] - 0s 269us/step - loss: 0.5298 - acc: 0.7271 - val_loss: 0.5453 - val_acc: 0.7425\n",
      "Epoch 130/2000\n",
      "623/623 [==============================] - 0s 200us/step - loss: 0.5257 - acc: 0.7319 - val_loss: 0.5433 - val_acc: 0.7388\n",
      "Epoch 131/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.5412 - acc: 0.7207 - val_loss: 0.5421 - val_acc: 0.7463\n",
      "Epoch 132/2000\n",
      "623/623 [==============================] - 0s 204us/step - loss: 0.5380 - acc: 0.7384 - val_loss: 0.5375 - val_acc: 0.7463\n",
      "Epoch 133/2000\n",
      "623/623 [==============================] - 0s 266us/step - loss: 0.5188 - acc: 0.7303 - val_loss: 0.5398 - val_acc: 0.7463\n",
      "Epoch 134/2000\n",
      "623/623 [==============================] - 0s 268us/step - loss: 0.5345 - acc: 0.7191 - val_loss: 0.5426 - val_acc: 0.7425\n",
      "Epoch 135/2000\n",
      "623/623 [==============================] - 0s 248us/step - loss: 0.5250 - acc: 0.7319 - val_loss: 0.5364 - val_acc: 0.7500\n",
      "Epoch 136/2000\n",
      "623/623 [==============================] - 0s 253us/step - loss: 0.5245 - acc: 0.7223 - val_loss: 0.5390 - val_acc: 0.7500\n",
      "Epoch 137/2000\n",
      "623/623 [==============================] - 0s 257us/step - loss: 0.5242 - acc: 0.7079 - val_loss: 0.5385 - val_acc: 0.7425\n",
      "Epoch 138/2000\n",
      "623/623 [==============================] - 0s 277us/step - loss: 0.5133 - acc: 0.7368 - val_loss: 0.5352 - val_acc: 0.7537\n",
      "Epoch 139/2000\n",
      "623/623 [==============================] - 0s 249us/step - loss: 0.5150 - acc: 0.7384 - val_loss: 0.5379 - val_acc: 0.7537\n",
      "Epoch 140/2000\n",
      "623/623 [==============================] - 0s 279us/step - loss: 0.5288 - acc: 0.7159 - val_loss: 0.5370 - val_acc: 0.7463\n",
      "Epoch 141/2000\n",
      "623/623 [==============================] - 0s 280us/step - loss: 0.5167 - acc: 0.7400 - val_loss: 0.5435 - val_acc: 0.7500\n",
      "Epoch 142/2000\n",
      "623/623 [==============================] - 0s 271us/step - loss: 0.5333 - acc: 0.7352 - val_loss: 0.5375 - val_acc: 0.7463\n",
      "Epoch 143/2000\n",
      "623/623 [==============================] - 0s 220us/step - loss: 0.5318 - acc: 0.7335 - val_loss: 0.5375 - val_acc: 0.7425\n",
      "Epoch 144/2000\n",
      "623/623 [==============================] - 0s 228us/step - loss: 0.5292 - acc: 0.7223 - val_loss: 0.5348 - val_acc: 0.7463\n",
      "Epoch 145/2000\n",
      "623/623 [==============================] - 0s 248us/step - loss: 0.5215 - acc: 0.7335 - val_loss: 0.5335 - val_acc: 0.7500\n",
      "Epoch 146/2000\n",
      "623/623 [==============================] - 0s 216us/step - loss: 0.5266 - acc: 0.7352 - val_loss: 0.5402 - val_acc: 0.7425\n",
      "Epoch 147/2000\n",
      "623/623 [==============================] - 0s 214us/step - loss: 0.5185 - acc: 0.7352 - val_loss: 0.5417 - val_acc: 0.7463\n",
      "Epoch 148/2000\n",
      "623/623 [==============================] - 0s 244us/step - loss: 0.5269 - acc: 0.7223 - val_loss: 0.5430 - val_acc: 0.7537\n",
      "Epoch 149/2000\n",
      "623/623 [==============================] - 0s 264us/step - loss: 0.5132 - acc: 0.7368 - val_loss: 0.5352 - val_acc: 0.7425\n",
      "Epoch 150/2000\n",
      "623/623 [==============================] - 0s 264us/step - loss: 0.5194 - acc: 0.7384 - val_loss: 0.5293 - val_acc: 0.7463\n",
      "Epoch 151/2000\n",
      "623/623 [==============================] - 0s 253us/step - loss: 0.5302 - acc: 0.7271 - val_loss: 0.5308 - val_acc: 0.7388\n",
      "Epoch 152/2000\n",
      "623/623 [==============================] - 0s 258us/step - loss: 0.5188 - acc: 0.7384 - val_loss: 0.5246 - val_acc: 0.7612\n",
      "Epoch 153/2000\n",
      "623/623 [==============================] - 0s 279us/step - loss: 0.5218 - acc: 0.7319 - val_loss: 0.5313 - val_acc: 0.7463\n",
      "Epoch 154/2000\n",
      "623/623 [==============================] - 0s 272us/step - loss: 0.5126 - acc: 0.7416 - val_loss: 0.5247 - val_acc: 0.7575\n",
      "Epoch 155/2000\n",
      "623/623 [==============================] - 0s 281us/step - loss: 0.5225 - acc: 0.7335 - val_loss: 0.5228 - val_acc: 0.7612\n",
      "Epoch 156/2000\n",
      "623/623 [==============================] - 0s 247us/step - loss: 0.5214 - acc: 0.7255 - val_loss: 0.5246 - val_acc: 0.7500\n",
      "Epoch 157/2000\n",
      "623/623 [==============================] - 0s 250us/step - loss: 0.5216 - acc: 0.7464 - val_loss: 0.5320 - val_acc: 0.7500\n",
      "Epoch 158/2000\n",
      "623/623 [==============================] - 0s 238us/step - loss: 0.5195 - acc: 0.7464 - val_loss: 0.5372 - val_acc: 0.7388\n",
      "Epoch 159/2000\n",
      "623/623 [==============================] - 0s 258us/step - loss: 0.5179 - acc: 0.7416 - val_loss: 0.5355 - val_acc: 0.7388\n",
      "Epoch 160/2000\n",
      "623/623 [==============================] - 0s 268us/step - loss: 0.5214 - acc: 0.7432 - val_loss: 0.5356 - val_acc: 0.7388\n",
      "Epoch 161/2000\n",
      "623/623 [==============================] - 0s 290us/step - loss: 0.5206 - acc: 0.7400 - val_loss: 0.5295 - val_acc: 0.7463\n",
      "Epoch 162/2000\n",
      "623/623 [==============================] - 0s 238us/step - loss: 0.5222 - acc: 0.7512 - val_loss: 0.5256 - val_acc: 0.7463\n",
      "Epoch 163/2000\n",
      "623/623 [==============================] - 0s 236us/step - loss: 0.5216 - acc: 0.7255 - val_loss: 0.5296 - val_acc: 0.7500\n",
      "Epoch 164/2000\n",
      "623/623 [==============================] - 0s 213us/step - loss: 0.5152 - acc: 0.7448 - val_loss: 0.5365 - val_acc: 0.7425\n",
      "Epoch 165/2000\n",
      "623/623 [==============================] - 0s 224us/step - loss: 0.5255 - acc: 0.7432 - val_loss: 0.5342 - val_acc: 0.7425\n",
      "Epoch 166/2000\n",
      "623/623 [==============================] - 0s 245us/step - loss: 0.5182 - acc: 0.7448 - val_loss: 0.5335 - val_acc: 0.7425\n",
      "Epoch 167/2000\n",
      "623/623 [==============================] - 0s 258us/step - loss: 0.5228 - acc: 0.7255 - val_loss: 0.5359 - val_acc: 0.7425\n",
      "Epoch 168/2000\n",
      "623/623 [==============================] - 0s 261us/step - loss: 0.5239 - acc: 0.7400 - val_loss: 0.5338 - val_acc: 0.7463\n",
      "Epoch 169/2000\n",
      "623/623 [==============================] - 0s 239us/step - loss: 0.5233 - acc: 0.7400 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 170/2000\n",
      "623/623 [==============================] - 0s 248us/step - loss: 0.5093 - acc: 0.7352 - val_loss: 0.5311 - val_acc: 0.7500\n",
      "Epoch 171/2000\n",
      "623/623 [==============================] - 0s 233us/step - loss: 0.5231 - acc: 0.7352 - val_loss: 0.5291 - val_acc: 0.7425\n",
      "Epoch 172/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.5138 - acc: 0.7432 - val_loss: 0.5230 - val_acc: 0.7500\n",
      "Epoch 173/2000\n",
      "623/623 [==============================] - 0s 270us/step - loss: 0.5062 - acc: 0.7416 - val_loss: 0.5288 - val_acc: 0.7537\n",
      "Epoch 174/2000\n",
      "623/623 [==============================] - 0s 250us/step - loss: 0.5248 - acc: 0.7368 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 175/2000\n",
      "623/623 [==============================] - 0s 243us/step - loss: 0.5118 - acc: 0.7400 - val_loss: 0.5372 - val_acc: 0.7463\n",
      "Epoch 176/2000\n",
      "623/623 [==============================] - 0s 248us/step - loss: 0.5228 - acc: 0.7560 - val_loss: 0.5368 - val_acc: 0.7388\n",
      "Epoch 177/2000\n",
      "623/623 [==============================] - 0s 268us/step - loss: 0.5302 - acc: 0.7255 - val_loss: 0.5359 - val_acc: 0.7463\n",
      "Epoch 178/2000\n",
      "623/623 [==============================] - 0s 264us/step - loss: 0.5187 - acc: 0.7448 - val_loss: 0.5322 - val_acc: 0.7425\n",
      "Epoch 179/2000\n",
      "623/623 [==============================] - 0s 272us/step - loss: 0.5236 - acc: 0.7352 - val_loss: 0.5260 - val_acc: 0.7500\n",
      "Epoch 180/2000\n",
      "623/623 [==============================] - 0s 212us/step - loss: 0.5140 - acc: 0.7416 - val_loss: 0.5251 - val_acc: 0.7537\n",
      "Epoch 181/2000\n",
      "623/623 [==============================] - 0s 229us/step - loss: 0.5196 - acc: 0.7223 - val_loss: 0.5305 - val_acc: 0.7500\n",
      "Epoch 182/2000\n",
      "623/623 [==============================] - 0s 230us/step - loss: 0.5018 - acc: 0.7544 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 183/2000\n",
      "623/623 [==============================] - 0s 209us/step - loss: 0.5225 - acc: 0.7303 - val_loss: 0.5274 - val_acc: 0.7537\n",
      "Epoch 184/2000\n",
      "623/623 [==============================] - 0s 250us/step - loss: 0.5123 - acc: 0.7480 - val_loss: 0.5287 - val_acc: 0.7612\n",
      "Epoch 185/2000\n",
      "623/623 [==============================] - 0s 249us/step - loss: 0.5164 - acc: 0.7432 - val_loss: 0.5270 - val_acc: 0.7575\n",
      "Epoch 186/2000\n",
      "623/623 [==============================] - 0s 244us/step - loss: 0.5119 - acc: 0.7384 - val_loss: 0.5212 - val_acc: 0.7575\n",
      "Epoch 187/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.5061 - acc: 0.7560 - val_loss: 0.5333 - val_acc: 0.7500\n",
      "Epoch 188/2000\n",
      "623/623 [==============================] - 0s 238us/step - loss: 0.5175 - acc: 0.7352 - val_loss: 0.5260 - val_acc: 0.7500\n",
      "Epoch 189/2000\n",
      "623/623 [==============================] - 0s 225us/step - loss: 0.5169 - acc: 0.7368 - val_loss: 0.5281 - val_acc: 0.7612\n",
      "Epoch 190/2000\n",
      "623/623 [==============================] - 0s 264us/step - loss: 0.5184 - acc: 0.7560 - val_loss: 0.5300 - val_acc: 0.7612\n",
      "Epoch 191/2000\n",
      "623/623 [==============================] - 0s 247us/step - loss: 0.5193 - acc: 0.7239 - val_loss: 0.5224 - val_acc: 0.7537\n",
      "Epoch 192/2000\n",
      "623/623 [==============================] - 0s 265us/step - loss: 0.5063 - acc: 0.7560 - val_loss: 0.5246 - val_acc: 0.7575\n",
      "Epoch 193/2000\n",
      "623/623 [==============================] - 0s 222us/step - loss: 0.5130 - acc: 0.7319 - val_loss: 0.5253 - val_acc: 0.7575\n",
      "Epoch 194/2000\n",
      "623/623 [==============================] - 0s 236us/step - loss: 0.5171 - acc: 0.7416 - val_loss: 0.5261 - val_acc: 0.7537\n",
      "Epoch 195/2000\n",
      "623/623 [==============================] - 0s 215us/step - loss: 0.5073 - acc: 0.7352 - val_loss: 0.5291 - val_acc: 0.7612\n",
      "Epoch 196/2000\n",
      "623/623 [==============================] - 0s 219us/step - loss: 0.5171 - acc: 0.7512 - val_loss: 0.5225 - val_acc: 0.7612\n",
      "Epoch 197/2000\n",
      "623/623 [==============================] - 0s 207us/step - loss: 0.5149 - acc: 0.7335 - val_loss: 0.5268 - val_acc: 0.7649\n",
      "Epoch 198/2000\n",
      "623/623 [==============================] - 0s 257us/step - loss: 0.5118 - acc: 0.7319 - val_loss: 0.5294 - val_acc: 0.7575\n",
      "Epoch 199/2000\n",
      "623/623 [==============================] - 0s 205us/step - loss: 0.5151 - acc: 0.7416 - val_loss: 0.5297 - val_acc: 0.7575\n",
      "Epoch 200/2000\n",
      "623/623 [==============================] - 0s 210us/step - loss: 0.5132 - acc: 0.7335 - val_loss: 0.5296 - val_acc: 0.7649\n",
      "Epoch 201/2000\n",
      "623/623 [==============================] - 0s 251us/step - loss: 0.5115 - acc: 0.7416 - val_loss: 0.5320 - val_acc: 0.7500\n",
      "Epoch 202/2000\n",
      "623/623 [==============================] - 0s 220us/step - loss: 0.5111 - acc: 0.7464 - val_loss: 0.5340 - val_acc: 0.7537\n",
      "Epoch 203/2000\n",
      "623/623 [==============================] - 0s 224us/step - loss: 0.5134 - acc: 0.7432 - val_loss: 0.5312 - val_acc: 0.7537\n",
      "Epoch 204/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.4977 - acc: 0.7657 - val_loss: 0.5381 - val_acc: 0.7575\n",
      "Epoch 205/2000\n",
      "623/623 [==============================] - 0s 206us/step - loss: 0.5066 - acc: 0.7400 - val_loss: 0.5386 - val_acc: 0.7537\n",
      "Epoch 206/2000\n",
      "623/623 [==============================] - 0s 269us/step - loss: 0.5146 - acc: 0.7400 - val_loss: 0.5339 - val_acc: 0.7537\n",
      "Epoch 207/2000\n",
      "623/623 [==============================] - 0s 270us/step - loss: 0.5040 - acc: 0.7448 - val_loss: 0.5293 - val_acc: 0.7575\n",
      "Epoch 208/2000\n",
      "623/623 [==============================] - 0s 224us/step - loss: 0.4930 - acc: 0.7560 - val_loss: 0.5271 - val_acc: 0.7500\n",
      "Epoch 209/2000\n",
      "623/623 [==============================] - 0s 277us/step - loss: 0.4961 - acc: 0.7464 - val_loss: 0.5256 - val_acc: 0.7575\n",
      "Epoch 210/2000\n",
      "623/623 [==============================] - 0s 214us/step - loss: 0.5095 - acc: 0.7448 - val_loss: 0.5350 - val_acc: 0.7575\n",
      "Epoch 211/2000\n",
      "623/623 [==============================] - 0s 258us/step - loss: 0.5020 - acc: 0.7448 - val_loss: 0.5253 - val_acc: 0.7575\n",
      "Epoch 212/2000\n",
      "623/623 [==============================] - 0s 258us/step - loss: 0.5139 - acc: 0.7480 - val_loss: 0.5276 - val_acc: 0.7537\n",
      "Epoch 213/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.5018 - acc: 0.7512 - val_loss: 0.5255 - val_acc: 0.7537\n",
      "Epoch 214/2000\n",
      "623/623 [==============================] - 0s 252us/step - loss: 0.5103 - acc: 0.7384 - val_loss: 0.5261 - val_acc: 0.7537\n",
      "Epoch 215/2000\n",
      "623/623 [==============================] - 0s 261us/step - loss: 0.4977 - acc: 0.7528 - val_loss: 0.5311 - val_acc: 0.7612\n",
      "Epoch 216/2000\n",
      "623/623 [==============================] - 0s 266us/step - loss: 0.5114 - acc: 0.7528 - val_loss: 0.5292 - val_acc: 0.7537\n",
      "Epoch 217/2000\n",
      "623/623 [==============================] - 0s 277us/step - loss: 0.5075 - acc: 0.7544 - val_loss: 0.5353 - val_acc: 0.7537\n",
      "Epoch 218/2000\n",
      "623/623 [==============================] - 0s 273us/step - loss: 0.4981 - acc: 0.7640 - val_loss: 0.5288 - val_acc: 0.7575\n",
      "Epoch 219/2000\n",
      "623/623 [==============================] - 0s 285us/step - loss: 0.5088 - acc: 0.7400 - val_loss: 0.5279 - val_acc: 0.7537\n",
      "Epoch 220/2000\n",
      "623/623 [==============================] - 0s 254us/step - loss: 0.5056 - acc: 0.7512 - val_loss: 0.5322 - val_acc: 0.7575\n",
      "Epoch 221/2000\n",
      "623/623 [==============================] - 0s 224us/step - loss: 0.4997 - acc: 0.7432 - val_loss: 0.5250 - val_acc: 0.7612\n",
      "Epoch 222/2000\n",
      "623/623 [==============================] - 0s 234us/step - loss: 0.5041 - acc: 0.7416 - val_loss: 0.5390 - val_acc: 0.7575\n",
      "Epoch 223/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.5034 - acc: 0.7512 - val_loss: 0.5270 - val_acc: 0.7500\n",
      "Epoch 224/2000\n",
      "623/623 [==============================] - 0s 250us/step - loss: 0.4985 - acc: 0.7432 - val_loss: 0.5189 - val_acc: 0.7575\n",
      "Epoch 225/2000\n",
      "623/623 [==============================] - 0s 258us/step - loss: 0.5107 - acc: 0.7560 - val_loss: 0.5228 - val_acc: 0.7537\n",
      "Epoch 226/2000\n",
      "623/623 [==============================] - 0s 275us/step - loss: 0.5095 - acc: 0.7416 - val_loss: 0.5223 - val_acc: 0.7500\n",
      "Epoch 227/2000\n",
      "623/623 [==============================] - 0s 269us/step - loss: 0.4961 - acc: 0.7544 - val_loss: 0.5249 - val_acc: 0.7537\n",
      "Epoch 228/2000\n",
      "623/623 [==============================] - 0s 229us/step - loss: 0.5100 - acc: 0.7416 - val_loss: 0.5258 - val_acc: 0.7575\n",
      "Epoch 229/2000\n",
      "623/623 [==============================] - 0s 244us/step - loss: 0.5099 - acc: 0.7432 - val_loss: 0.5164 - val_acc: 0.7575\n",
      "Epoch 230/2000\n",
      "623/623 [==============================] - 0s 272us/step - loss: 0.4986 - acc: 0.7464 - val_loss: 0.5193 - val_acc: 0.7537\n",
      "Epoch 231/2000\n",
      "623/623 [==============================] - 0s 295us/step - loss: 0.4997 - acc: 0.7544 - val_loss: 0.5233 - val_acc: 0.7575\n",
      "Epoch 232/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.5117 - acc: 0.7448 - val_loss: 0.5278 - val_acc: 0.7500\n",
      "Epoch 233/2000\n",
      "623/623 [==============================] - 0s 264us/step - loss: 0.5018 - acc: 0.7592 - val_loss: 0.5194 - val_acc: 0.7575\n",
      "Epoch 234/2000\n",
      "623/623 [==============================] - 0s 294us/step - loss: 0.5170 - acc: 0.7239 - val_loss: 0.5237 - val_acc: 0.7575\n",
      "Epoch 235/2000\n",
      "623/623 [==============================] - 0s 276us/step - loss: 0.4956 - acc: 0.7496 - val_loss: 0.5167 - val_acc: 0.7575\n",
      "Epoch 236/2000\n",
      "623/623 [==============================] - 0s 258us/step - loss: 0.4950 - acc: 0.7576 - val_loss: 0.5177 - val_acc: 0.7575\n",
      "Epoch 237/2000\n",
      "623/623 [==============================] - 0s 251us/step - loss: 0.5109 - acc: 0.7271 - val_loss: 0.5246 - val_acc: 0.7575\n",
      "Epoch 238/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623/623 [==============================] - 0s 273us/step - loss: 0.5041 - acc: 0.7544 - val_loss: 0.5277 - val_acc: 0.7575\n",
      "Epoch 239/2000\n",
      "623/623 [==============================] - 0s 262us/step - loss: 0.5000 - acc: 0.7448 - val_loss: 0.5239 - val_acc: 0.7537\n",
      "Epoch 240/2000\n",
      "623/623 [==============================] - 0s 266us/step - loss: 0.4898 - acc: 0.7608 - val_loss: 0.5254 - val_acc: 0.7537\n",
      "Epoch 241/2000\n",
      "623/623 [==============================] - 0s 266us/step - loss: 0.4892 - acc: 0.7608 - val_loss: 0.5232 - val_acc: 0.7537\n",
      "Epoch 242/2000\n",
      "623/623 [==============================] - 0s 336us/step - loss: 0.4968 - acc: 0.7624 - val_loss: 0.5226 - val_acc: 0.7537\n",
      "Epoch 243/2000\n",
      "623/623 [==============================] - 0s 249us/step - loss: 0.4930 - acc: 0.7528 - val_loss: 0.5188 - val_acc: 0.7575\n",
      "Epoch 244/2000\n",
      "623/623 [==============================] - 0s 269us/step - loss: 0.4973 - acc: 0.7448 - val_loss: 0.5254 - val_acc: 0.7537\n",
      "Epoch 245/2000\n",
      "623/623 [==============================] - 0s 230us/step - loss: 0.4975 - acc: 0.7608 - val_loss: 0.5264 - val_acc: 0.7537\n",
      "Epoch 246/2000\n",
      "623/623 [==============================] - 0s 219us/step - loss: 0.5044 - acc: 0.7608 - val_loss: 0.5209 - val_acc: 0.7463\n",
      "Epoch 247/2000\n",
      "623/623 [==============================] - 0s 303us/step - loss: 0.4857 - acc: 0.7640 - val_loss: 0.5271 - val_acc: 0.7537\n",
      "Epoch 248/2000\n",
      "623/623 [==============================] - 0s 288us/step - loss: 0.5014 - acc: 0.7544 - val_loss: 0.5199 - val_acc: 0.7575\n",
      "Epoch 249/2000\n",
      "623/623 [==============================] - 0s 268us/step - loss: 0.4920 - acc: 0.7384 - val_loss: 0.5190 - val_acc: 0.7537\n",
      "Epoch 250/2000\n",
      "623/623 [==============================] - 0s 277us/step - loss: 0.5051 - acc: 0.7464 - val_loss: 0.5202 - val_acc: 0.7537\n",
      "Epoch 251/2000\n",
      "623/623 [==============================] - 0s 270us/step - loss: 0.4967 - acc: 0.7480 - val_loss: 0.5235 - val_acc: 0.7537\n",
      "Epoch 252/2000\n",
      "623/623 [==============================] - 0s 274us/step - loss: 0.4981 - acc: 0.7689 - val_loss: 0.5243 - val_acc: 0.7575\n",
      "Epoch 253/2000\n",
      "623/623 [==============================] - 0s 293us/step - loss: 0.4913 - acc: 0.7432 - val_loss: 0.5230 - val_acc: 0.7575\n",
      "Epoch 254/2000\n",
      "623/623 [==============================] - 0s 262us/step - loss: 0.5043 - acc: 0.7448 - val_loss: 0.5211 - val_acc: 0.7575\n",
      "Epoch 255/2000\n",
      "623/623 [==============================] - 0s 256us/step - loss: 0.5093 - acc: 0.7448 - val_loss: 0.5199 - val_acc: 0.7649\n",
      "Epoch 256/2000\n",
      "623/623 [==============================] - 0s 255us/step - loss: 0.4908 - acc: 0.7608 - val_loss: 0.5188 - val_acc: 0.7687\n",
      "Epoch 257/2000\n",
      "623/623 [==============================] - 0s 276us/step - loss: 0.4942 - acc: 0.7432 - val_loss: 0.5176 - val_acc: 0.7612\n",
      "Epoch 258/2000\n",
      "623/623 [==============================] - 0s 258us/step - loss: 0.5003 - acc: 0.7384 - val_loss: 0.5187 - val_acc: 0.7612\n",
      "Epoch 259/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4938 - acc: 0.7576 - val_loss: 0.5110 - val_acc: 0.7575\n",
      "Epoch 260/2000\n",
      "623/623 [==============================] - 0s 252us/step - loss: 0.4914 - acc: 0.7608 - val_loss: 0.5119 - val_acc: 0.7575\n",
      "Epoch 261/2000\n",
      "623/623 [==============================] - 0s 291us/step - loss: 0.5038 - acc: 0.7400 - val_loss: 0.5182 - val_acc: 0.7612\n",
      "Epoch 262/2000\n",
      "623/623 [==============================] - 0s 237us/step - loss: 0.4965 - acc: 0.7624 - val_loss: 0.5179 - val_acc: 0.7575\n",
      "Epoch 263/2000\n",
      "623/623 [==============================] - ETA: 0s - loss: 0.5020 - acc: 0.758 - 0s 252us/step - loss: 0.4977 - acc: 0.7576 - val_loss: 0.5230 - val_acc: 0.7575\n",
      "Epoch 264/2000\n",
      "623/623 [==============================] - 0s 263us/step - loss: 0.5013 - acc: 0.7544 - val_loss: 0.5213 - val_acc: 0.7612\n",
      "Epoch 265/2000\n",
      "623/623 [==============================] - 0s 283us/step - loss: 0.4905 - acc: 0.7528 - val_loss: 0.5204 - val_acc: 0.7575\n",
      "Epoch 266/2000\n",
      "623/623 [==============================] - 0s 272us/step - loss: 0.5099 - acc: 0.7560 - val_loss: 0.5184 - val_acc: 0.7575\n",
      "Epoch 267/2000\n",
      "623/623 [==============================] - 0s 251us/step - loss: 0.5023 - acc: 0.7400 - val_loss: 0.5208 - val_acc: 0.7612\n",
      "Epoch 268/2000\n",
      "623/623 [==============================] - 0s 265us/step - loss: 0.4956 - acc: 0.7608 - val_loss: 0.5240 - val_acc: 0.7575\n",
      "Epoch 269/2000\n",
      "623/623 [==============================] - 0s 286us/step - loss: 0.4908 - acc: 0.7544 - val_loss: 0.5167 - val_acc: 0.7575\n",
      "Epoch 270/2000\n",
      "623/623 [==============================] - 0s 275us/step - loss: 0.4949 - acc: 0.7560 - val_loss: 0.5124 - val_acc: 0.7612\n",
      "Epoch 271/2000\n",
      "623/623 [==============================] - 0s 283us/step - loss: 0.5044 - acc: 0.7352 - val_loss: 0.5104 - val_acc: 0.7537\n",
      "Epoch 272/2000\n",
      "623/623 [==============================] - 0s 257us/step - loss: 0.4998 - acc: 0.7560 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 273/2000\n",
      "623/623 [==============================] - 0s 260us/step - loss: 0.4999 - acc: 0.7448 - val_loss: 0.5166 - val_acc: 0.7575\n",
      "Epoch 274/2000\n",
      "623/623 [==============================] - 0s 285us/step - loss: 0.4956 - acc: 0.7528 - val_loss: 0.5243 - val_acc: 0.7612\n",
      "Epoch 275/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4808 - acc: 0.7705 - val_loss: 0.5188 - val_acc: 0.7612\n",
      "Epoch 276/2000\n",
      "623/623 [==============================] - 0s 234us/step - loss: 0.4990 - acc: 0.7480 - val_loss: 0.5190 - val_acc: 0.7575\n",
      "Epoch 277/2000\n",
      "623/623 [==============================] - 0s 240us/step - loss: 0.4721 - acc: 0.7673 - val_loss: 0.5183 - val_acc: 0.7649\n",
      "Epoch 278/2000\n",
      "623/623 [==============================] - 0s 234us/step - loss: 0.4846 - acc: 0.7496 - val_loss: 0.5167 - val_acc: 0.7537\n",
      "Epoch 279/2000\n",
      "623/623 [==============================] - 0s 306us/step - loss: 0.5004 - acc: 0.7528 - val_loss: 0.5165 - val_acc: 0.7612\n",
      "Epoch 280/2000\n",
      "623/623 [==============================] - 0s 256us/step - loss: 0.4992 - acc: 0.7319 - val_loss: 0.5193 - val_acc: 0.7537\n",
      "Epoch 281/2000\n",
      "623/623 [==============================] - 0s 260us/step - loss: 0.4906 - acc: 0.7464 - val_loss: 0.5224 - val_acc: 0.7612\n",
      "Epoch 282/2000\n",
      "623/623 [==============================] - 0s 273us/step - loss: 0.5004 - acc: 0.7689 - val_loss: 0.5276 - val_acc: 0.7500\n",
      "Epoch 283/2000\n",
      "623/623 [==============================] - 0s 266us/step - loss: 0.4916 - acc: 0.7640 - val_loss: 0.5225 - val_acc: 0.7612\n",
      "Epoch 284/2000\n",
      "623/623 [==============================] - 0s 233us/step - loss: 0.4835 - acc: 0.7560 - val_loss: 0.5163 - val_acc: 0.7500\n",
      "Epoch 285/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.5030 - acc: 0.7528 - val_loss: 0.5155 - val_acc: 0.7612\n",
      "Epoch 286/2000\n",
      "623/623 [==============================] - 0s 259us/step - loss: 0.4853 - acc: 0.7737 - val_loss: 0.5180 - val_acc: 0.7612\n",
      "Epoch 287/2000\n",
      "623/623 [==============================] - 0s 248us/step - loss: 0.4818 - acc: 0.7721 - val_loss: 0.5279 - val_acc: 0.7612\n",
      "Epoch 288/2000\n",
      "623/623 [==============================] - 0s 222us/step - loss: 0.4811 - acc: 0.7528 - val_loss: 0.5259 - val_acc: 0.7761\n",
      "Epoch 289/2000\n",
      "623/623 [==============================] - 0s 250us/step - loss: 0.4872 - acc: 0.7624 - val_loss: 0.5181 - val_acc: 0.7649\n",
      "Epoch 290/2000\n",
      "623/623 [==============================] - 0s 244us/step - loss: 0.4891 - acc: 0.7560 - val_loss: 0.5194 - val_acc: 0.7575\n",
      "Epoch 291/2000\n",
      "623/623 [==============================] - 0s 225us/step - loss: 0.4801 - acc: 0.7817 - val_loss: 0.5218 - val_acc: 0.7649\n",
      "Epoch 292/2000\n",
      "623/623 [==============================] - 0s 209us/step - loss: 0.4927 - acc: 0.7528 - val_loss: 0.5172 - val_acc: 0.7612\n",
      "Epoch 293/2000\n",
      "623/623 [==============================] - 0s 229us/step - loss: 0.4788 - acc: 0.7608 - val_loss: 0.5169 - val_acc: 0.7537\n",
      "Epoch 294/2000\n",
      "623/623 [==============================] - 0s 278us/step - loss: 0.4906 - acc: 0.7592 - val_loss: 0.5178 - val_acc: 0.7575\n",
      "Epoch 295/2000\n",
      "623/623 [==============================] - 0s 226us/step - loss: 0.4860 - acc: 0.7705 - val_loss: 0.5222 - val_acc: 0.7537\n",
      "Epoch 296/2000\n",
      "623/623 [==============================] - 0s 253us/step - loss: 0.4880 - acc: 0.7657 - val_loss: 0.5256 - val_acc: 0.7575\n",
      "Epoch 297/2000\n",
      "623/623 [==============================] - 0s 238us/step - loss: 0.4835 - acc: 0.7576 - val_loss: 0.5134 - val_acc: 0.7537\n",
      "Epoch 298/2000\n",
      "623/623 [==============================] - 0s 256us/step - loss: 0.4960 - acc: 0.7512 - val_loss: 0.5183 - val_acc: 0.7575\n",
      "Epoch 299/2000\n",
      "623/623 [==============================] - 0s 253us/step - loss: 0.4999 - acc: 0.7512 - val_loss: 0.5175 - val_acc: 0.7537\n",
      "Epoch 300/2000\n",
      "623/623 [==============================] - 0s 292us/step - loss: 0.4895 - acc: 0.7673 - val_loss: 0.5147 - val_acc: 0.7649\n",
      "Epoch 301/2000\n",
      "623/623 [==============================] - 0s 252us/step - loss: 0.4828 - acc: 0.7705 - val_loss: 0.5175 - val_acc: 0.7649\n",
      "Epoch 302/2000\n",
      "623/623 [==============================] - 0s 220us/step - loss: 0.4883 - acc: 0.7544 - val_loss: 0.5221 - val_acc: 0.7687\n",
      "Epoch 303/2000\n",
      "623/623 [==============================] - 0s 286us/step - loss: 0.4927 - acc: 0.7673 - val_loss: 0.5174 - val_acc: 0.7761\n",
      "Epoch 304/2000\n",
      "623/623 [==============================] - 0s 251us/step - loss: 0.4750 - acc: 0.7737 - val_loss: 0.5159 - val_acc: 0.7724\n",
      "Epoch 305/2000\n",
      "623/623 [==============================] - 0s 268us/step - loss: 0.4887 - acc: 0.7480 - val_loss: 0.5274 - val_acc: 0.7724\n",
      "Epoch 306/2000\n",
      "623/623 [==============================] - 0s 275us/step - loss: 0.4664 - acc: 0.7657 - val_loss: 0.5180 - val_acc: 0.7575\n",
      "Epoch 307/2000\n",
      "623/623 [==============================] - 0s 287us/step - loss: 0.4915 - acc: 0.7624 - val_loss: 0.5159 - val_acc: 0.7575\n",
      "Epoch 308/2000\n",
      "623/623 [==============================] - 0s 260us/step - loss: 0.4717 - acc: 0.7753 - val_loss: 0.5142 - val_acc: 0.7537\n",
      "Epoch 309/2000\n",
      "623/623 [==============================] - 0s 301us/step - loss: 0.4736 - acc: 0.7785 - val_loss: 0.5121 - val_acc: 0.7799\n",
      "Epoch 310/2000\n",
      "623/623 [==============================] - 0s 228us/step - loss: 0.4848 - acc: 0.7624 - val_loss: 0.5257 - val_acc: 0.7575\n",
      "Epoch 311/2000\n",
      "623/623 [==============================] - 0s 209us/step - loss: 0.4836 - acc: 0.7528 - val_loss: 0.5169 - val_acc: 0.7649\n",
      "Epoch 312/2000\n",
      "623/623 [==============================] - 0s 230us/step - loss: 0.4970 - acc: 0.7592 - val_loss: 0.5071 - val_acc: 0.7575\n",
      "Epoch 313/2000\n",
      "623/623 [==============================] - 0s 266us/step - loss: 0.4854 - acc: 0.7640 - val_loss: 0.5097 - val_acc: 0.7575\n",
      "Epoch 314/2000\n",
      "623/623 [==============================] - 0s 250us/step - loss: 0.4895 - acc: 0.7657 - val_loss: 0.5138 - val_acc: 0.7649\n",
      "Epoch 315/2000\n",
      "623/623 [==============================] - 0s 250us/step - loss: 0.4801 - acc: 0.7560 - val_loss: 0.5253 - val_acc: 0.7724\n",
      "Epoch 316/2000\n",
      "623/623 [==============================] - 0s 295us/step - loss: 0.4803 - acc: 0.7737 - val_loss: 0.5266 - val_acc: 0.7575\n",
      "Epoch 317/2000\n",
      "623/623 [==============================] - 0s 263us/step - loss: 0.4813 - acc: 0.7592 - val_loss: 0.5084 - val_acc: 0.7761\n",
      "Epoch 318/2000\n",
      "623/623 [==============================] - 0s 267us/step - loss: 0.4973 - acc: 0.7608 - val_loss: 0.5076 - val_acc: 0.7724\n",
      "Epoch 319/2000\n",
      "623/623 [==============================] - 0s 253us/step - loss: 0.4788 - acc: 0.7689 - val_loss: 0.5057 - val_acc: 0.7612\n",
      "Epoch 320/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.4884 - acc: 0.7560 - val_loss: 0.5199 - val_acc: 0.7724\n",
      "Epoch 321/2000\n",
      "623/623 [==============================] - 0s 255us/step - loss: 0.4814 - acc: 0.7528 - val_loss: 0.5155 - val_acc: 0.7724\n",
      "Epoch 322/2000\n",
      "623/623 [==============================] - 0s 221us/step - loss: 0.4908 - acc: 0.7737 - val_loss: 0.5047 - val_acc: 0.7687\n",
      "Epoch 323/2000\n",
      "623/623 [==============================] - 0s 301us/step - loss: 0.4913 - acc: 0.7817 - val_loss: 0.5047 - val_acc: 0.7649\n",
      "Epoch 324/2000\n",
      "623/623 [==============================] - 0s 294us/step - loss: 0.4761 - acc: 0.7657 - val_loss: 0.5179 - val_acc: 0.7463\n",
      "Epoch 325/2000\n",
      "623/623 [==============================] - 0s 291us/step - loss: 0.4811 - acc: 0.7769 - val_loss: 0.4999 - val_acc: 0.7649\n",
      "Epoch 326/2000\n",
      "623/623 [==============================] - 0s 275us/step - loss: 0.4848 - acc: 0.7705 - val_loss: 0.4988 - val_acc: 0.7724\n",
      "Epoch 327/2000\n",
      "623/623 [==============================] - 0s 260us/step - loss: 0.4744 - acc: 0.7721 - val_loss: 0.4997 - val_acc: 0.7537\n",
      "Epoch 328/2000\n",
      "623/623 [==============================] - 0s 254us/step - loss: 0.4879 - acc: 0.7673 - val_loss: 0.5008 - val_acc: 0.7575\n",
      "Epoch 329/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.4763 - acc: 0.7544 - val_loss: 0.5077 - val_acc: 0.7836\n",
      "Epoch 330/2000\n",
      "623/623 [==============================] - 0s 263us/step - loss: 0.4810 - acc: 0.7737 - val_loss: 0.5033 - val_acc: 0.7724\n",
      "Epoch 331/2000\n",
      "623/623 [==============================] - 0s 257us/step - loss: 0.4794 - acc: 0.7673 - val_loss: 0.5161 - val_acc: 0.7649\n",
      "Epoch 332/2000\n",
      "623/623 [==============================] - 0s 266us/step - loss: 0.4850 - acc: 0.7640 - val_loss: 0.5092 - val_acc: 0.7649\n",
      "Epoch 333/2000\n",
      "623/623 [==============================] - 0s 263us/step - loss: 0.4819 - acc: 0.7689 - val_loss: 0.5057 - val_acc: 0.7612\n",
      "Epoch 334/2000\n",
      "623/623 [==============================] - 0s 250us/step - loss: 0.4743 - acc: 0.7624 - val_loss: 0.5037 - val_acc: 0.7649\n",
      "Epoch 335/2000\n",
      "623/623 [==============================] - 0s 239us/step - loss: 0.4755 - acc: 0.7689 - val_loss: 0.5090 - val_acc: 0.7761\n",
      "Epoch 336/2000\n",
      "623/623 [==============================] - 0s 211us/step - loss: 0.4579 - acc: 0.7801 - val_loss: 0.5183 - val_acc: 0.7612\n",
      "Epoch 337/2000\n",
      "623/623 [==============================] - 0s 228us/step - loss: 0.4848 - acc: 0.7544 - val_loss: 0.5098 - val_acc: 0.7724\n",
      "Epoch 338/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.4753 - acc: 0.7737 - val_loss: 0.5040 - val_acc: 0.7612\n",
      "Epoch 339/2000\n",
      "623/623 [==============================] - 0s 235us/step - loss: 0.4759 - acc: 0.7785 - val_loss: 0.5039 - val_acc: 0.7649\n",
      "Epoch 340/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4688 - acc: 0.7769 - val_loss: 0.5076 - val_acc: 0.7612\n",
      "Epoch 341/2000\n",
      "623/623 [==============================] - 0s 256us/step - loss: 0.4747 - acc: 0.7705 - val_loss: 0.5133 - val_acc: 0.7612\n",
      "Epoch 342/2000\n",
      "623/623 [==============================] - 0s 244us/step - loss: 0.4799 - acc: 0.7705 - val_loss: 0.5093 - val_acc: 0.7612\n",
      "Epoch 343/2000\n",
      "623/623 [==============================] - 0s 275us/step - loss: 0.4872 - acc: 0.7785 - val_loss: 0.5042 - val_acc: 0.7612\n",
      "Epoch 344/2000\n",
      "623/623 [==============================] - 0s 234us/step - loss: 0.4887 - acc: 0.7657 - val_loss: 0.5179 - val_acc: 0.7575\n",
      "Epoch 345/2000\n",
      "623/623 [==============================] - 0s 234us/step - loss: 0.4710 - acc: 0.7705 - val_loss: 0.5150 - val_acc: 0.7612\n",
      "Epoch 346/2000\n",
      "623/623 [==============================] - 0s 254us/step - loss: 0.4676 - acc: 0.7737 - val_loss: 0.5044 - val_acc: 0.7649\n",
      "Epoch 347/2000\n",
      "623/623 [==============================] - 0s 215us/step - loss: 0.4609 - acc: 0.7897 - val_loss: 0.5141 - val_acc: 0.7537\n",
      "Epoch 348/2000\n",
      "623/623 [==============================] - 0s 212us/step - loss: 0.4874 - acc: 0.7849 - val_loss: 0.5055 - val_acc: 0.7575\n",
      "Epoch 349/2000\n",
      "623/623 [==============================] - 0s 211us/step - loss: 0.4788 - acc: 0.7769 - val_loss: 0.5076 - val_acc: 0.7649\n",
      "Epoch 350/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.4867 - acc: 0.7640 - val_loss: 0.5107 - val_acc: 0.7537\n",
      "Epoch 351/2000\n",
      "623/623 [==============================] - 0s 251us/step - loss: 0.4805 - acc: 0.7705 - val_loss: 0.5038 - val_acc: 0.7575\n",
      "Epoch 352/2000\n",
      "623/623 [==============================] - 0s 256us/step - loss: 0.4733 - acc: 0.7785 - val_loss: 0.5278 - val_acc: 0.7425\n",
      "Epoch 353/2000\n",
      "623/623 [==============================] - 0s 266us/step - loss: 0.4578 - acc: 0.7929 - val_loss: 0.5033 - val_acc: 0.7575\n",
      "Epoch 354/2000\n",
      "623/623 [==============================] - 0s 244us/step - loss: 0.4671 - acc: 0.7897 - val_loss: 0.5071 - val_acc: 0.7649\n",
      "Epoch 355/2000\n",
      "623/623 [==============================] - 0s 259us/step - loss: 0.4658 - acc: 0.7785 - val_loss: 0.4995 - val_acc: 0.7724\n",
      "Epoch 356/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623/623 [==============================] - 0s 256us/step - loss: 0.4744 - acc: 0.7865 - val_loss: 0.5023 - val_acc: 0.7799\n",
      "Epoch 357/2000\n",
      "623/623 [==============================] - 0s 254us/step - loss: 0.4725 - acc: 0.7721 - val_loss: 0.5040 - val_acc: 0.7724\n",
      "Epoch 358/2000\n",
      "623/623 [==============================] - 0s 243us/step - loss: 0.4661 - acc: 0.7769 - val_loss: 0.5062 - val_acc: 0.7761\n",
      "Epoch 359/2000\n",
      "623/623 [==============================] - 0s 251us/step - loss: 0.4663 - acc: 0.7929 - val_loss: 0.5080 - val_acc: 0.7687\n",
      "Epoch 360/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.4759 - acc: 0.7689 - val_loss: 0.5056 - val_acc: 0.7687\n",
      "Epoch 361/2000\n",
      "623/623 [==============================] - 0s 200us/step - loss: 0.4576 - acc: 0.7897 - val_loss: 0.5025 - val_acc: 0.7687\n",
      "Epoch 362/2000\n",
      "623/623 [==============================] - 0s 232us/step - loss: 0.4863 - acc: 0.7769 - val_loss: 0.5067 - val_acc: 0.7724\n",
      "Epoch 363/2000\n",
      "623/623 [==============================] - 0s 213us/step - loss: 0.4658 - acc: 0.7865 - val_loss: 0.4979 - val_acc: 0.7761\n",
      "Epoch 364/2000\n",
      "623/623 [==============================] - 0s 228us/step - loss: 0.4810 - acc: 0.7753 - val_loss: 0.5046 - val_acc: 0.7761\n",
      "Epoch 365/2000\n",
      "623/623 [==============================] - 0s 220us/step - loss: 0.4734 - acc: 0.7961 - val_loss: 0.5060 - val_acc: 0.7537\n",
      "Epoch 366/2000\n",
      "623/623 [==============================] - 0s 257us/step - loss: 0.4680 - acc: 0.7945 - val_loss: 0.5117 - val_acc: 0.7687\n",
      "Epoch 367/2000\n",
      "623/623 [==============================] - 0s 264us/step - loss: 0.4579 - acc: 0.7929 - val_loss: 0.5123 - val_acc: 0.7724\n",
      "Epoch 368/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.4602 - acc: 0.7994 - val_loss: 0.5015 - val_acc: 0.7537\n",
      "Epoch 369/2000\n",
      "623/623 [==============================] - 0s 222us/step - loss: 0.4774 - acc: 0.7833 - val_loss: 0.5026 - val_acc: 0.7687\n",
      "Epoch 370/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.4518 - acc: 0.7961 - val_loss: 0.5060 - val_acc: 0.7649\n",
      "Epoch 371/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.4628 - acc: 0.7961 - val_loss: 0.5120 - val_acc: 0.7612\n",
      "Epoch 372/2000\n",
      "623/623 [==============================] - 0s 206us/step - loss: 0.4565 - acc: 0.7945 - val_loss: 0.4987 - val_acc: 0.7575\n",
      "Epoch 373/2000\n",
      "623/623 [==============================] - 0s 205us/step - loss: 0.4600 - acc: 0.7961 - val_loss: 0.5045 - val_acc: 0.7537\n",
      "Epoch 374/2000\n",
      "623/623 [==============================] - 0s 209us/step - loss: 0.4530 - acc: 0.7945 - val_loss: 0.5051 - val_acc: 0.7649\n",
      "Epoch 375/2000\n",
      "623/623 [==============================] - 0s 203us/step - loss: 0.4769 - acc: 0.7721 - val_loss: 0.5043 - val_acc: 0.7537\n",
      "Epoch 376/2000\n",
      "623/623 [==============================] - 0s 191us/step - loss: 0.4688 - acc: 0.7753 - val_loss: 0.5055 - val_acc: 0.7612\n",
      "Epoch 377/2000\n",
      "623/623 [==============================] - 0s 213us/step - loss: 0.4782 - acc: 0.7657 - val_loss: 0.4982 - val_acc: 0.7575\n",
      "Epoch 378/2000\n",
      "623/623 [==============================] - 0s 187us/step - loss: 0.4682 - acc: 0.7817 - val_loss: 0.5187 - val_acc: 0.7425\n",
      "Epoch 379/2000\n",
      "623/623 [==============================] - 0s 224us/step - loss: 0.4609 - acc: 0.7833 - val_loss: 0.5041 - val_acc: 0.7687\n",
      "Epoch 380/2000\n",
      "623/623 [==============================] - 0s 223us/step - loss: 0.4722 - acc: 0.7897 - val_loss: 0.5044 - val_acc: 0.7537\n",
      "Epoch 381/2000\n",
      "623/623 [==============================] - 0s 236us/step - loss: 0.4538 - acc: 0.7897 - val_loss: 0.5044 - val_acc: 0.7687\n",
      "Epoch 382/2000\n",
      "623/623 [==============================] - 0s 228us/step - loss: 0.4655 - acc: 0.7673 - val_loss: 0.4990 - val_acc: 0.7687\n",
      "Epoch 383/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.4705 - acc: 0.7657 - val_loss: 0.5038 - val_acc: 0.7649\n",
      "Epoch 384/2000\n",
      "623/623 [==============================] - 0s 221us/step - loss: 0.4695 - acc: 0.7881 - val_loss: 0.5055 - val_acc: 0.7724\n",
      "Epoch 385/2000\n",
      "623/623 [==============================] - 0s 225us/step - loss: 0.4833 - acc: 0.7801 - val_loss: 0.5084 - val_acc: 0.7687\n",
      "Epoch 386/2000\n",
      "623/623 [==============================] - 0s 222us/step - loss: 0.4613 - acc: 0.7833 - val_loss: 0.5383 - val_acc: 0.7127\n",
      "Epoch 387/2000\n",
      "623/623 [==============================] - 0s 214us/step - loss: 0.4756 - acc: 0.7624 - val_loss: 0.5059 - val_acc: 0.7649\n",
      "Epoch 388/2000\n",
      "623/623 [==============================] - 0s 284us/step - loss: 0.4609 - acc: 0.7913 - val_loss: 0.5037 - val_acc: 0.7687\n",
      "Epoch 389/2000\n",
      "623/623 [==============================] - 0s 236us/step - loss: 0.4806 - acc: 0.7721 - val_loss: 0.5071 - val_acc: 0.7687\n",
      "Epoch 390/2000\n",
      "623/623 [==============================] - 0s 234us/step - loss: 0.4493 - acc: 0.7961 - val_loss: 0.5061 - val_acc: 0.7724\n",
      "Epoch 391/2000\n",
      "623/623 [==============================] - 0s 239us/step - loss: 0.4702 - acc: 0.7817 - val_loss: 0.5166 - val_acc: 0.7575\n",
      "Epoch 392/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4666 - acc: 0.7897 - val_loss: 0.5085 - val_acc: 0.7575\n",
      "Epoch 393/2000\n",
      "623/623 [==============================] - 0s 271us/step - loss: 0.4629 - acc: 0.7801 - val_loss: 0.5038 - val_acc: 0.7724\n",
      "Epoch 394/2000\n",
      "623/623 [==============================] - 0s 271us/step - loss: 0.4674 - acc: 0.7865 - val_loss: 0.5010 - val_acc: 0.7724\n",
      "Epoch 395/2000\n",
      "623/623 [==============================] - 0s 243us/step - loss: 0.4621 - acc: 0.7785 - val_loss: 0.5022 - val_acc: 0.7724\n",
      "Epoch 396/2000\n",
      "623/623 [==============================] - 0s 279us/step - loss: 0.4529 - acc: 0.7913 - val_loss: 0.5024 - val_acc: 0.7687\n",
      "Epoch 397/2000\n",
      "623/623 [==============================] - 0s 286us/step - loss: 0.4569 - acc: 0.7913 - val_loss: 0.5029 - val_acc: 0.7687\n",
      "Epoch 398/2000\n",
      "623/623 [==============================] - 0s 269us/step - loss: 0.4534 - acc: 0.8170 - val_loss: 0.5051 - val_acc: 0.7649\n",
      "Epoch 399/2000\n",
      "623/623 [==============================] - 0s 260us/step - loss: 0.4559 - acc: 0.7978 - val_loss: 0.5110 - val_acc: 0.7649\n",
      "Epoch 400/2000\n",
      "623/623 [==============================] - 0s 253us/step - loss: 0.4517 - acc: 0.7929 - val_loss: 0.5087 - val_acc: 0.7649\n",
      "Epoch 401/2000\n",
      "623/623 [==============================] - 0s 244us/step - loss: 0.4510 - acc: 0.7978 - val_loss: 0.5004 - val_acc: 0.7724\n",
      "Epoch 402/2000\n",
      "623/623 [==============================] - 0s 232us/step - loss: 0.4646 - acc: 0.7929 - val_loss: 0.5157 - val_acc: 0.7463\n",
      "Epoch 403/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4580 - acc: 0.7897 - val_loss: 0.5022 - val_acc: 0.7724\n",
      "Epoch 404/2000\n",
      "623/623 [==============================] - 0s 269us/step - loss: 0.4567 - acc: 0.7913 - val_loss: 0.5096 - val_acc: 0.7537\n",
      "Epoch 405/2000\n",
      "623/623 [==============================] - 0s 275us/step - loss: 0.4499 - acc: 0.8010 - val_loss: 0.4972 - val_acc: 0.7724\n",
      "Epoch 406/2000\n",
      "623/623 [==============================] - 0s 245us/step - loss: 0.4666 - acc: 0.7881 - val_loss: 0.4973 - val_acc: 0.7724\n",
      "Epoch 407/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4569 - acc: 0.7978 - val_loss: 0.4999 - val_acc: 0.7724\n",
      "Epoch 408/2000\n",
      "623/623 [==============================] - 0s 223us/step - loss: 0.4488 - acc: 0.8026 - val_loss: 0.5077 - val_acc: 0.7687\n",
      "Epoch 409/2000\n",
      "623/623 [==============================] - 0s 232us/step - loss: 0.4573 - acc: 0.7849 - val_loss: 0.5004 - val_acc: 0.7649\n",
      "Epoch 410/2000\n",
      "623/623 [==============================] - 0s 254us/step - loss: 0.4605 - acc: 0.7817 - val_loss: 0.5025 - val_acc: 0.7612\n",
      "Epoch 411/2000\n",
      "623/623 [==============================] - 0s 207us/step - loss: 0.4640 - acc: 0.7929 - val_loss: 0.5154 - val_acc: 0.7463\n",
      "Epoch 412/2000\n",
      "623/623 [==============================] - 0s 199us/step - loss: 0.4432 - acc: 0.8090 - val_loss: 0.5126 - val_acc: 0.7537\n",
      "Epoch 413/2000\n",
      "623/623 [==============================] - 0s 215us/step - loss: 0.4554 - acc: 0.7881 - val_loss: 0.5011 - val_acc: 0.7687\n",
      "Epoch 414/2000\n",
      "623/623 [==============================] - 0s 217us/step - loss: 0.4552 - acc: 0.8026 - val_loss: 0.5053 - val_acc: 0.7575\n",
      "Epoch 415/2000\n",
      "623/623 [==============================] - 0s 230us/step - loss: 0.4620 - acc: 0.7833 - val_loss: 0.4923 - val_acc: 0.7724\n",
      "Epoch 416/2000\n",
      "623/623 [==============================] - 0s 206us/step - loss: 0.4550 - acc: 0.8010 - val_loss: 0.4974 - val_acc: 0.7687\n",
      "Epoch 417/2000\n",
      "623/623 [==============================] - 0s 214us/step - loss: 0.4503 - acc: 0.8026 - val_loss: 0.4889 - val_acc: 0.7724\n",
      "Epoch 418/2000\n",
      "623/623 [==============================] - 0s 219us/step - loss: 0.4701 - acc: 0.7945 - val_loss: 0.4900 - val_acc: 0.7724\n",
      "Epoch 419/2000\n",
      "623/623 [==============================] - 0s 223us/step - loss: 0.4490 - acc: 0.7945 - val_loss: 0.4964 - val_acc: 0.7612\n",
      "Epoch 420/2000\n",
      "623/623 [==============================] - 0s 251us/step - loss: 0.4538 - acc: 0.7929 - val_loss: 0.4967 - val_acc: 0.7649\n",
      "Epoch 421/2000\n",
      "623/623 [==============================] - 0s 236us/step - loss: 0.4628 - acc: 0.7817 - val_loss: 0.5018 - val_acc: 0.7687\n",
      "Epoch 422/2000\n",
      "623/623 [==============================] - 0s 278us/step - loss: 0.4537 - acc: 0.7978 - val_loss: 0.5006 - val_acc: 0.7724\n",
      "Epoch 423/2000\n",
      "623/623 [==============================] - 0s 258us/step - loss: 0.4429 - acc: 0.8122 - val_loss: 0.5017 - val_acc: 0.7724\n",
      "Epoch 424/2000\n",
      "623/623 [==============================] - 0s 224us/step - loss: 0.4689 - acc: 0.7929 - val_loss: 0.5054 - val_acc: 0.7649\n",
      "Epoch 425/2000\n",
      "623/623 [==============================] - 0s 221us/step - loss: 0.4440 - acc: 0.7961 - val_loss: 0.5093 - val_acc: 0.7575\n",
      "Epoch 426/2000\n",
      "623/623 [==============================] - 0s 232us/step - loss: 0.4612 - acc: 0.7833 - val_loss: 0.5022 - val_acc: 0.7724\n",
      "Epoch 427/2000\n",
      "623/623 [==============================] - 0s 238us/step - loss: 0.4626 - acc: 0.8058 - val_loss: 0.4998 - val_acc: 0.7687\n",
      "Epoch 428/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.4545 - acc: 0.7961 - val_loss: 0.5008 - val_acc: 0.7724\n",
      "Epoch 429/2000\n",
      "623/623 [==============================] - 0s 268us/step - loss: 0.4411 - acc: 0.8026 - val_loss: 0.4997 - val_acc: 0.7761\n",
      "Epoch 430/2000\n",
      "623/623 [==============================] - 0s 232us/step - loss: 0.4467 - acc: 0.7978 - val_loss: 0.5112 - val_acc: 0.7463\n",
      "Epoch 431/2000\n",
      "623/623 [==============================] - 0s 224us/step - loss: 0.4559 - acc: 0.8138 - val_loss: 0.4991 - val_acc: 0.7649\n",
      "Epoch 432/2000\n",
      "623/623 [==============================] - 0s 233us/step - loss: 0.4419 - acc: 0.8010 - val_loss: 0.4926 - val_acc: 0.7761\n",
      "Epoch 433/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.4554 - acc: 0.7994 - val_loss: 0.4924 - val_acc: 0.7687\n",
      "Epoch 434/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.4539 - acc: 0.7961 - val_loss: 0.5238 - val_acc: 0.7276\n",
      "Epoch 435/2000\n",
      "623/623 [==============================] - 0s 255us/step - loss: 0.4498 - acc: 0.7849 - val_loss: 0.4953 - val_acc: 0.7724\n",
      "Epoch 436/2000\n",
      "623/623 [==============================] - 0s 230us/step - loss: 0.4492 - acc: 0.7897 - val_loss: 0.5007 - val_acc: 0.7687\n",
      "Epoch 437/2000\n",
      "623/623 [==============================] - 0s 229us/step - loss: 0.4497 - acc: 0.7961 - val_loss: 0.4946 - val_acc: 0.7761\n",
      "Epoch 438/2000\n",
      "623/623 [==============================] - 0s 218us/step - loss: 0.4463 - acc: 0.7817 - val_loss: 0.4927 - val_acc: 0.7724\n",
      "Epoch 439/2000\n",
      "623/623 [==============================] - 0s 272us/step - loss: 0.4461 - acc: 0.8026 - val_loss: 0.4983 - val_acc: 0.7724\n",
      "Epoch 440/2000\n",
      "623/623 [==============================] - 0s 247us/step - loss: 0.4480 - acc: 0.8026 - val_loss: 0.4927 - val_acc: 0.7724\n",
      "Epoch 441/2000\n",
      "623/623 [==============================] - 0s 236us/step - loss: 0.4529 - acc: 0.7929 - val_loss: 0.5057 - val_acc: 0.7687\n",
      "Epoch 442/2000\n",
      "623/623 [==============================] - 0s 245us/step - loss: 0.4481 - acc: 0.7929 - val_loss: 0.5053 - val_acc: 0.7612\n",
      "Epoch 443/2000\n",
      "623/623 [==============================] - 0s 235us/step - loss: 0.4417 - acc: 0.7961 - val_loss: 0.4971 - val_acc: 0.7687\n",
      "Epoch 444/2000\n",
      "623/623 [==============================] - 0s 205us/step - loss: 0.4492 - acc: 0.8074 - val_loss: 0.4951 - val_acc: 0.7687\n",
      "Epoch 445/2000\n",
      "623/623 [==============================] - 0s 252us/step - loss: 0.4386 - acc: 0.8122 - val_loss: 0.4958 - val_acc: 0.7724\n",
      "Epoch 446/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.4383 - acc: 0.8010 - val_loss: 0.5024 - val_acc: 0.7687\n",
      "Epoch 447/2000\n",
      "623/623 [==============================] - 0s 205us/step - loss: 0.4438 - acc: 0.8106 - val_loss: 0.5012 - val_acc: 0.7687\n",
      "Epoch 448/2000\n",
      "623/623 [==============================] - 0s 237us/step - loss: 0.4478 - acc: 0.8122 - val_loss: 0.4960 - val_acc: 0.7687\n",
      "Epoch 449/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.4451 - acc: 0.8074 - val_loss: 0.5004 - val_acc: 0.7612\n",
      "Epoch 450/2000\n",
      "623/623 [==============================] - 0s 225us/step - loss: 0.4511 - acc: 0.7913 - val_loss: 0.4930 - val_acc: 0.7724\n",
      "Epoch 451/2000\n",
      "623/623 [==============================] - 0s 248us/step - loss: 0.4361 - acc: 0.8010 - val_loss: 0.4954 - val_acc: 0.7724\n",
      "Epoch 452/2000\n",
      "623/623 [==============================] - 0s 218us/step - loss: 0.4525 - acc: 0.8074 - val_loss: 0.4987 - val_acc: 0.7724\n",
      "Epoch 453/2000\n",
      "623/623 [==============================] - 0s 259us/step - loss: 0.4540 - acc: 0.8058 - val_loss: 0.4946 - val_acc: 0.7687\n",
      "Epoch 454/2000\n",
      "623/623 [==============================] - 0s 257us/step - loss: 0.4383 - acc: 0.7978 - val_loss: 0.4988 - val_acc: 0.7612\n",
      "Epoch 455/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4506 - acc: 0.7881 - val_loss: 0.4902 - val_acc: 0.7687\n",
      "Epoch 456/2000\n",
      "623/623 [==============================] - 0s 265us/step - loss: 0.4519 - acc: 0.7945 - val_loss: 0.4921 - val_acc: 0.7649\n",
      "Epoch 457/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.4637 - acc: 0.7881 - val_loss: 0.4898 - val_acc: 0.7724\n",
      "Epoch 458/2000\n",
      "623/623 [==============================] - 0s 269us/step - loss: 0.4336 - acc: 0.8042 - val_loss: 0.4896 - val_acc: 0.7724\n",
      "Epoch 459/2000\n",
      "623/623 [==============================] - 0s 261us/step - loss: 0.4430 - acc: 0.8042 - val_loss: 0.4938 - val_acc: 0.7649\n",
      "Epoch 460/2000\n",
      "623/623 [==============================] - 0s 255us/step - loss: 0.4428 - acc: 0.8042 - val_loss: 0.4979 - val_acc: 0.7649\n",
      "Epoch 461/2000\n",
      "623/623 [==============================] - 0s 228us/step - loss: 0.4519 - acc: 0.7881 - val_loss: 0.4945 - val_acc: 0.7724\n",
      "Epoch 462/2000\n",
      "623/623 [==============================] - 0s 275us/step - loss: 0.4313 - acc: 0.8010 - val_loss: 0.5048 - val_acc: 0.7575\n",
      "Epoch 463/2000\n",
      "623/623 [==============================] - 0s 235us/step - loss: 0.4468 - acc: 0.8058 - val_loss: 0.4931 - val_acc: 0.7687\n",
      "Epoch 464/2000\n",
      "623/623 [==============================] - 0s 259us/step - loss: 0.4312 - acc: 0.8170 - val_loss: 0.4934 - val_acc: 0.7687\n",
      "Epoch 465/2000\n",
      "623/623 [==============================] - 0s 247us/step - loss: 0.4564 - acc: 0.7961 - val_loss: 0.4913 - val_acc: 0.7724\n",
      "Epoch 466/2000\n",
      "623/623 [==============================] - 0s 255us/step - loss: 0.4328 - acc: 0.8074 - val_loss: 0.5012 - val_acc: 0.7687\n",
      "Epoch 467/2000\n",
      "623/623 [==============================] - 0s 218us/step - loss: 0.4487 - acc: 0.7994 - val_loss: 0.5004 - val_acc: 0.7687\n",
      "Epoch 468/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.4337 - acc: 0.8042 - val_loss: 0.4948 - val_acc: 0.7761\n",
      "Epoch 469/2000\n",
      "623/623 [==============================] - 0s 292us/step - loss: 0.4440 - acc: 0.8010 - val_loss: 0.4934 - val_acc: 0.7724\n",
      "Epoch 470/2000\n",
      "623/623 [==============================] - 0s 251us/step - loss: 0.4414 - acc: 0.8010 - val_loss: 0.5214 - val_acc: 0.7313\n",
      "Epoch 471/2000\n",
      "623/623 [==============================] - 0s 269us/step - loss: 0.4376 - acc: 0.7978 - val_loss: 0.4918 - val_acc: 0.7761\n",
      "Epoch 472/2000\n",
      "623/623 [==============================] - 0s 210us/step - loss: 0.4385 - acc: 0.7961 - val_loss: 0.4912 - val_acc: 0.7724\n",
      "Epoch 473/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.4341 - acc: 0.7929 - val_loss: 0.4979 - val_acc: 0.7687\n",
      "Epoch 474/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623/623 [==============================] - 0s 290us/step - loss: 0.4408 - acc: 0.8026 - val_loss: 0.4959 - val_acc: 0.7724\n",
      "Epoch 475/2000\n",
      "623/623 [==============================] - 0s 284us/step - loss: 0.4407 - acc: 0.8026 - val_loss: 0.4945 - val_acc: 0.7724\n",
      "Epoch 476/2000\n",
      "623/623 [==============================] - 0s 267us/step - loss: 0.4504 - acc: 0.8026 - val_loss: 0.4932 - val_acc: 0.7724\n",
      "Epoch 477/2000\n",
      "623/623 [==============================] - 0s 254us/step - loss: 0.4500 - acc: 0.7978 - val_loss: 0.4966 - val_acc: 0.7724\n",
      "Epoch 478/2000\n",
      "623/623 [==============================] - 0s 225us/step - loss: 0.4468 - acc: 0.7994 - val_loss: 0.4957 - val_acc: 0.7724\n",
      "Epoch 479/2000\n",
      "623/623 [==============================] - 0s 271us/step - loss: 0.4507 - acc: 0.7913 - val_loss: 0.4935 - val_acc: 0.7724\n",
      "Epoch 480/2000\n",
      "623/623 [==============================] - 0s 252us/step - loss: 0.4385 - acc: 0.8074 - val_loss: 0.4980 - val_acc: 0.7799\n",
      "Epoch 481/2000\n",
      "623/623 [==============================] - 0s 237us/step - loss: 0.4439 - acc: 0.7994 - val_loss: 0.4928 - val_acc: 0.7724\n",
      "Epoch 482/2000\n",
      "623/623 [==============================] - 0s 269us/step - loss: 0.4480 - acc: 0.8074 - val_loss: 0.4924 - val_acc: 0.7724\n",
      "Epoch 483/2000\n",
      "623/623 [==============================] - 0s 222us/step - loss: 0.4418 - acc: 0.8010 - val_loss: 0.5005 - val_acc: 0.7724\n",
      "Epoch 484/2000\n",
      "623/623 [==============================] - 0s 259us/step - loss: 0.4436 - acc: 0.8074 - val_loss: 0.4977 - val_acc: 0.7687\n",
      "Epoch 485/2000\n",
      "623/623 [==============================] - 0s 283us/step - loss: 0.4419 - acc: 0.7978 - val_loss: 0.5112 - val_acc: 0.7575\n",
      "Epoch 486/2000\n",
      "623/623 [==============================] - 0s 215us/step - loss: 0.4458 - acc: 0.7945 - val_loss: 0.4981 - val_acc: 0.7761\n",
      "Epoch 487/2000\n",
      "623/623 [==============================] - ETA: 0s - loss: 0.4456 - acc: 0.808 - 0s 270us/step - loss: 0.4465 - acc: 0.8026 - val_loss: 0.5024 - val_acc: 0.7724\n",
      "Epoch 488/2000\n",
      "623/623 [==============================] - 0s 233us/step - loss: 0.4336 - acc: 0.7978 - val_loss: 0.4990 - val_acc: 0.7724\n",
      "Epoch 489/2000\n",
      "623/623 [==============================] - 0s 250us/step - loss: 0.4383 - acc: 0.8010 - val_loss: 0.4991 - val_acc: 0.7761\n",
      "Epoch 490/2000\n",
      "623/623 [==============================] - 0s 263us/step - loss: 0.4454 - acc: 0.7945 - val_loss: 0.4985 - val_acc: 0.7761\n",
      "Epoch 491/2000\n",
      "623/623 [==============================] - 0s 237us/step - loss: 0.4368 - acc: 0.8042 - val_loss: 0.5061 - val_acc: 0.7724\n",
      "Epoch 492/2000\n",
      "623/623 [==============================] - 0s 270us/step - loss: 0.4283 - acc: 0.8218 - val_loss: 0.5017 - val_acc: 0.7687\n",
      "Epoch 493/2000\n",
      "623/623 [==============================] - 0s 244us/step - loss: 0.4374 - acc: 0.8042 - val_loss: 0.5015 - val_acc: 0.7761\n",
      "Epoch 494/2000\n",
      "623/623 [==============================] - 0s 232us/step - loss: 0.4468 - acc: 0.7961 - val_loss: 0.5006 - val_acc: 0.7799\n",
      "Epoch 495/2000\n",
      "623/623 [==============================] - 0s 211us/step - loss: 0.4387 - acc: 0.8186 - val_loss: 0.5095 - val_acc: 0.7649\n",
      "Epoch 496/2000\n",
      "623/623 [==============================] - 0s 208us/step - loss: 0.4422 - acc: 0.7978 - val_loss: 0.4953 - val_acc: 0.7761\n",
      "Epoch 497/2000\n",
      "623/623 [==============================] - 0s 220us/step - loss: 0.4416 - acc: 0.8010 - val_loss: 0.4953 - val_acc: 0.7761\n",
      "Epoch 498/2000\n",
      "623/623 [==============================] - 0s 230us/step - loss: 0.4338 - acc: 0.8090 - val_loss: 0.4984 - val_acc: 0.7761\n",
      "Epoch 499/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.4453 - acc: 0.8042 - val_loss: 0.5000 - val_acc: 0.7761\n",
      "Epoch 500/2000\n",
      "623/623 [==============================] - 0s 271us/step - loss: 0.4420 - acc: 0.7978 - val_loss: 0.5095 - val_acc: 0.7724\n",
      "Epoch 501/2000\n",
      "623/623 [==============================] - 0s 253us/step - loss: 0.4473 - acc: 0.7978 - val_loss: 0.5001 - val_acc: 0.7724\n",
      "Epoch 502/2000\n",
      "623/623 [==============================] - 0s 266us/step - loss: 0.4341 - acc: 0.8138 - val_loss: 0.4963 - val_acc: 0.7724\n",
      "Epoch 503/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.4394 - acc: 0.8170 - val_loss: 0.4955 - val_acc: 0.7761\n",
      "Epoch 504/2000\n",
      "623/623 [==============================] - 0s 216us/step - loss: 0.4300 - acc: 0.8042 - val_loss: 0.4968 - val_acc: 0.7761\n",
      "Epoch 505/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4409 - acc: 0.8042 - val_loss: 0.4935 - val_acc: 0.7799\n",
      "Epoch 506/2000\n",
      "623/623 [==============================] - 0s 244us/step - loss: 0.4440 - acc: 0.8106 - val_loss: 0.5015 - val_acc: 0.7724\n",
      "Epoch 507/2000\n",
      "623/623 [==============================] - 0s 252us/step - loss: 0.4308 - acc: 0.8106 - val_loss: 0.4956 - val_acc: 0.7687\n",
      "Epoch 508/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.4302 - acc: 0.8074 - val_loss: 0.5008 - val_acc: 0.7649\n",
      "Epoch 509/2000\n",
      "623/623 [==============================] - 0s 240us/step - loss: 0.4372 - acc: 0.8058 - val_loss: 0.5029 - val_acc: 0.7761\n",
      "Epoch 510/2000\n",
      "623/623 [==============================] - 0s 245us/step - loss: 0.4321 - acc: 0.8090 - val_loss: 0.4947 - val_acc: 0.7836\n",
      "Epoch 511/2000\n",
      "623/623 [==============================] - 0s 261us/step - loss: 0.4143 - acc: 0.8250 - val_loss: 0.4965 - val_acc: 0.7761\n",
      "Epoch 512/2000\n",
      "623/623 [==============================] - 0s 223us/step - loss: 0.4337 - acc: 0.8122 - val_loss: 0.4968 - val_acc: 0.7761\n",
      "Epoch 513/2000\n",
      "623/623 [==============================] - 0s 251us/step - loss: 0.4248 - acc: 0.8154 - val_loss: 0.4971 - val_acc: 0.7799\n",
      "Epoch 514/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4147 - acc: 0.8250 - val_loss: 0.5003 - val_acc: 0.7799\n",
      "Epoch 515/2000\n",
      "623/623 [==============================] - 0s 249us/step - loss: 0.4123 - acc: 0.8218 - val_loss: 0.5050 - val_acc: 0.7761\n",
      "Epoch 516/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4391 - acc: 0.8074 - val_loss: 0.4993 - val_acc: 0.7836\n",
      "Epoch 517/2000\n",
      "623/623 [==============================] - 0s 218us/step - loss: 0.4442 - acc: 0.8074 - val_loss: 0.4963 - val_acc: 0.7799\n",
      "Epoch 518/2000\n",
      "623/623 [==============================] - 0s 252us/step - loss: 0.4330 - acc: 0.8010 - val_loss: 0.4959 - val_acc: 0.7873\n",
      "Epoch 519/2000\n",
      "623/623 [==============================] - 0s 268us/step - loss: 0.4354 - acc: 0.8106 - val_loss: 0.4939 - val_acc: 0.7836\n",
      "Epoch 520/2000\n",
      "623/623 [==============================] - 0s 297us/step - loss: 0.4231 - acc: 0.8138 - val_loss: 0.4970 - val_acc: 0.7948\n",
      "Epoch 521/2000\n",
      "623/623 [==============================] - 0s 278us/step - loss: 0.4244 - acc: 0.8170 - val_loss: 0.5024 - val_acc: 0.7836\n",
      "Epoch 522/2000\n",
      "623/623 [==============================] - 0s 300us/step - loss: 0.4366 - acc: 0.8090 - val_loss: 0.4972 - val_acc: 0.7948\n",
      "Epoch 523/2000\n",
      "623/623 [==============================] - 0s 252us/step - loss: 0.4408 - acc: 0.8170 - val_loss: 0.4986 - val_acc: 0.7873\n",
      "Epoch 524/2000\n",
      "623/623 [==============================] - 0s 261us/step - loss: 0.4344 - acc: 0.7994 - val_loss: 0.5007 - val_acc: 0.7724\n",
      "Epoch 525/2000\n",
      "623/623 [==============================] - 0s 228us/step - loss: 0.4154 - acc: 0.8234 - val_loss: 0.4967 - val_acc: 0.7910\n",
      "Epoch 526/2000\n",
      "623/623 [==============================] - 0s 239us/step - loss: 0.4341 - acc: 0.8074 - val_loss: 0.4962 - val_acc: 0.7910\n",
      "Epoch 527/2000\n",
      "623/623 [==============================] - 0s 244us/step - loss: 0.4498 - acc: 0.7961 - val_loss: 0.4950 - val_acc: 0.7910\n",
      "Epoch 528/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4329 - acc: 0.8026 - val_loss: 0.5128 - val_acc: 0.7724\n",
      "Epoch 529/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4359 - acc: 0.8090 - val_loss: 0.4970 - val_acc: 0.7799\n",
      "Epoch 530/2000\n",
      "623/623 [==============================] - 0s 245us/step - loss: 0.4499 - acc: 0.8026 - val_loss: 0.4975 - val_acc: 0.7799\n",
      "Epoch 531/2000\n",
      "623/623 [==============================] - 0s 251us/step - loss: 0.4512 - acc: 0.7961 - val_loss: 0.4984 - val_acc: 0.7836\n",
      "Epoch 532/2000\n",
      "623/623 [==============================] - 0s 218us/step - loss: 0.4348 - acc: 0.8074 - val_loss: 0.4968 - val_acc: 0.7836\n",
      "Epoch 533/2000\n",
      "623/623 [==============================] - 0s 252us/step - loss: 0.4224 - acc: 0.8122 - val_loss: 0.4931 - val_acc: 0.7836\n",
      "Epoch 534/2000\n",
      "623/623 [==============================] - 0s 224us/step - loss: 0.4373 - acc: 0.8058 - val_loss: 0.4991 - val_acc: 0.7836\n",
      "Epoch 535/2000\n",
      "623/623 [==============================] - 0s 261us/step - loss: 0.4306 - acc: 0.8154 - val_loss: 0.4950 - val_acc: 0.7724\n",
      "Epoch 536/2000\n",
      "623/623 [==============================] - 0s 306us/step - loss: 0.4339 - acc: 0.8026 - val_loss: 0.4927 - val_acc: 0.7761\n",
      "Epoch 537/2000\n",
      "623/623 [==============================] - 0s 259us/step - loss: 0.4257 - acc: 0.8218 - val_loss: 0.4934 - val_acc: 0.7910\n",
      "Epoch 538/2000\n",
      "623/623 [==============================] - 0s 234us/step - loss: 0.4227 - acc: 0.8106 - val_loss: 0.4953 - val_acc: 0.7873\n",
      "Epoch 539/2000\n",
      "623/623 [==============================] - 0s 285us/step - loss: 0.4283 - acc: 0.8154 - val_loss: 0.4931 - val_acc: 0.7799\n",
      "Epoch 540/2000\n",
      "623/623 [==============================] - 0s 226us/step - loss: 0.4368 - acc: 0.8138 - val_loss: 0.4912 - val_acc: 0.7836\n",
      "Epoch 541/2000\n",
      "623/623 [==============================] - 0s 243us/step - loss: 0.4382 - acc: 0.8026 - val_loss: 0.4974 - val_acc: 0.7761\n",
      "Epoch 542/2000\n",
      "623/623 [==============================] - 0s 249us/step - loss: 0.4340 - acc: 0.8218 - val_loss: 0.4992 - val_acc: 0.7724\n",
      "Epoch 543/2000\n",
      "623/623 [==============================] - 0s 267us/step - loss: 0.4274 - acc: 0.8138 - val_loss: 0.4927 - val_acc: 0.7799\n",
      "Epoch 544/2000\n",
      "623/623 [==============================] - 0s 261us/step - loss: 0.4372 - acc: 0.8090 - val_loss: 0.5006 - val_acc: 0.7724\n",
      "Epoch 545/2000\n",
      "623/623 [==============================] - 0s 210us/step - loss: 0.4246 - acc: 0.8042 - val_loss: 0.4951 - val_acc: 0.7836\n",
      "Epoch 546/2000\n",
      "623/623 [==============================] - 0s 235us/step - loss: 0.4233 - acc: 0.8058 - val_loss: 0.4934 - val_acc: 0.7799\n",
      "Epoch 547/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.4194 - acc: 0.8218 - val_loss: 0.4999 - val_acc: 0.7724\n",
      "Epoch 548/2000\n",
      "623/623 [==============================] - 0s 253us/step - loss: 0.4347 - acc: 0.7994 - val_loss: 0.4995 - val_acc: 0.7799\n",
      "Epoch 549/2000\n",
      "623/623 [==============================] - 0s 205us/step - loss: 0.4233 - acc: 0.8074 - val_loss: 0.4975 - val_acc: 0.7836\n",
      "Epoch 550/2000\n",
      "623/623 [==============================] - 0s 235us/step - loss: 0.4345 - acc: 0.8074 - val_loss: 0.5134 - val_acc: 0.7612\n",
      "Epoch 551/2000\n",
      "623/623 [==============================] - 0s 243us/step - loss: 0.4312 - acc: 0.8170 - val_loss: 0.4945 - val_acc: 0.7910\n",
      "Epoch 552/2000\n",
      "623/623 [==============================] - 0s 247us/step - loss: 0.4282 - acc: 0.8218 - val_loss: 0.4979 - val_acc: 0.7836\n",
      "Epoch 553/2000\n",
      "623/623 [==============================] - 0s 237us/step - loss: 0.4218 - acc: 0.8283 - val_loss: 0.4976 - val_acc: 0.7799\n",
      "Epoch 554/2000\n",
      "623/623 [==============================] - ETA: 0s - loss: 0.4400 - acc: 0.812 - 0s 249us/step - loss: 0.4343 - acc: 0.8122 - val_loss: 0.5030 - val_acc: 0.7873\n",
      "Epoch 555/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.4342 - acc: 0.8106 - val_loss: 0.5007 - val_acc: 0.7761\n",
      "Epoch 556/2000\n",
      "623/623 [==============================] - 0s 212us/step - loss: 0.4199 - acc: 0.8331 - val_loss: 0.4984 - val_acc: 0.7873\n",
      "Epoch 557/2000\n",
      "623/623 [==============================] - 0s 200us/step - loss: 0.4306 - acc: 0.8010 - val_loss: 0.4953 - val_acc: 0.7873\n",
      "Epoch 558/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.4379 - acc: 0.7994 - val_loss: 0.4935 - val_acc: 0.7799\n",
      "Epoch 559/2000\n",
      "623/623 [==============================] - 0s 236us/step - loss: 0.4240 - acc: 0.8074 - val_loss: 0.4927 - val_acc: 0.7799\n",
      "Epoch 560/2000\n",
      "623/623 [==============================] - 0s 245us/step - loss: 0.4240 - acc: 0.8170 - val_loss: 0.4934 - val_acc: 0.7910\n",
      "Epoch 561/2000\n",
      "623/623 [==============================] - 0s 254us/step - loss: 0.4261 - acc: 0.8106 - val_loss: 0.4917 - val_acc: 0.7836\n",
      "Epoch 562/2000\n",
      "623/623 [==============================] - 0s 238us/step - loss: 0.4249 - acc: 0.8090 - val_loss: 0.4926 - val_acc: 0.7910\n",
      "Epoch 563/2000\n",
      "623/623 [==============================] - 0s 211us/step - loss: 0.4262 - acc: 0.8186 - val_loss: 0.4966 - val_acc: 0.7836\n",
      "Epoch 564/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.4189 - acc: 0.8154 - val_loss: 0.4942 - val_acc: 0.7910\n",
      "Epoch 565/2000\n",
      "623/623 [==============================] - 0s 233us/step - loss: 0.4243 - acc: 0.8074 - val_loss: 0.4933 - val_acc: 0.7985\n",
      "Epoch 566/2000\n",
      "623/623 [==============================] - 0s 229us/step - loss: 0.4245 - acc: 0.8106 - val_loss: 0.4944 - val_acc: 0.7948\n",
      "Epoch 567/2000\n",
      "623/623 [==============================] - 0s 266us/step - loss: 0.4361 - acc: 0.8074 - val_loss: 0.4987 - val_acc: 0.7910\n",
      "Epoch 568/2000\n",
      "623/623 [==============================] - 0s 228us/step - loss: 0.4322 - acc: 0.8186 - val_loss: 0.4957 - val_acc: 0.7836\n",
      "Epoch 569/2000\n",
      "623/623 [==============================] - 0s 223us/step - loss: 0.4134 - acc: 0.8234 - val_loss: 0.5057 - val_acc: 0.7724\n",
      "Epoch 570/2000\n",
      "623/623 [==============================] - 0s 239us/step - loss: 0.4288 - acc: 0.8250 - val_loss: 0.4968 - val_acc: 0.7910\n",
      "Epoch 571/2000\n",
      "623/623 [==============================] - 0s 236us/step - loss: 0.4214 - acc: 0.8106 - val_loss: 0.4983 - val_acc: 0.7836\n",
      "Epoch 572/2000\n",
      "623/623 [==============================] - 0s 259us/step - loss: 0.4227 - acc: 0.8138 - val_loss: 0.5015 - val_acc: 0.7873\n",
      "Epoch 573/2000\n",
      "623/623 [==============================] - 0s 249us/step - loss: 0.4157 - acc: 0.8122 - val_loss: 0.4984 - val_acc: 0.7948\n",
      "Epoch 574/2000\n",
      "623/623 [==============================] - 0s 219us/step - loss: 0.4273 - acc: 0.8042 - val_loss: 0.5010 - val_acc: 0.7910\n",
      "Epoch 575/2000\n",
      "623/623 [==============================] - 0s 253us/step - loss: 0.4142 - acc: 0.8154 - val_loss: 0.4949 - val_acc: 0.7873\n",
      "Epoch 576/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.4104 - acc: 0.8347 - val_loss: 0.4959 - val_acc: 0.7985\n",
      "Epoch 577/2000\n",
      "623/623 [==============================] - 0s 235us/step - loss: 0.4334 - acc: 0.8074 - val_loss: 0.4988 - val_acc: 0.7836\n",
      "Epoch 578/2000\n",
      "623/623 [==============================] - 0s 278us/step - loss: 0.4328 - acc: 0.8154 - val_loss: 0.4963 - val_acc: 0.7873\n",
      "Epoch 579/2000\n",
      "623/623 [==============================] - 0s 233us/step - loss: 0.4323 - acc: 0.8106 - val_loss: 0.4984 - val_acc: 0.7910\n",
      "Epoch 580/2000\n",
      "623/623 [==============================] - 0s 218us/step - loss: 0.4316 - acc: 0.8106 - val_loss: 0.4984 - val_acc: 0.7948\n",
      "Epoch 581/2000\n",
      "623/623 [==============================] - 0s 249us/step - loss: 0.4126 - acc: 0.8250 - val_loss: 0.4976 - val_acc: 0.7873\n",
      "Epoch 582/2000\n",
      "623/623 [==============================] - 0s 197us/step - loss: 0.4200 - acc: 0.8074 - val_loss: 0.4994 - val_acc: 0.7948\n",
      "Epoch 583/2000\n",
      "623/623 [==============================] - 0s 242us/step - loss: 0.4289 - acc: 0.8106 - val_loss: 0.5067 - val_acc: 0.7836\n",
      "Epoch 584/2000\n",
      "623/623 [==============================] - 0s 230us/step - loss: 0.4214 - acc: 0.8042 - val_loss: 0.5021 - val_acc: 0.7910\n",
      "Epoch 585/2000\n",
      "623/623 [==============================] - 0s 234us/step - loss: 0.4296 - acc: 0.8154 - val_loss: 0.4986 - val_acc: 0.7873\n",
      "Epoch 586/2000\n",
      "623/623 [==============================] - 0s 288us/step - loss: 0.4414 - acc: 0.8042 - val_loss: 0.4985 - val_acc: 0.7836\n",
      "Epoch 587/2000\n",
      "623/623 [==============================] - 0s 193us/step - loss: 0.4282 - acc: 0.8090 - val_loss: 0.4947 - val_acc: 0.7799\n",
      "Epoch 588/2000\n",
      "623/623 [==============================] - 0s 220us/step - loss: 0.4322 - acc: 0.8106 - val_loss: 0.4948 - val_acc: 0.7948\n",
      "Epoch 589/2000\n",
      "623/623 [==============================] - 0s 221us/step - loss: 0.4098 - acc: 0.8170 - val_loss: 0.5003 - val_acc: 0.7910\n",
      "Epoch 590/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.4186 - acc: 0.8122 - val_loss: 0.5000 - val_acc: 0.7948\n",
      "Epoch 591/2000\n",
      "623/623 [==============================] - 0s 253us/step - loss: 0.4142 - acc: 0.8250 - val_loss: 0.5108 - val_acc: 0.7724\n",
      "Epoch 592/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623/623 [==============================] - 0s 228us/step - loss: 0.4184 - acc: 0.8106 - val_loss: 0.5018 - val_acc: 0.7985\n",
      "Epoch 593/2000\n",
      "623/623 [==============================] - 0s 271us/step - loss: 0.3993 - acc: 0.8363 - val_loss: 0.5013 - val_acc: 0.8022\n",
      "Epoch 594/2000\n",
      "623/623 [==============================] - 0s 233us/step - loss: 0.4312 - acc: 0.8154 - val_loss: 0.5033 - val_acc: 0.7873\n",
      "Epoch 595/2000\n",
      "623/623 [==============================] - 0s 242us/step - loss: 0.4158 - acc: 0.8058 - val_loss: 0.5021 - val_acc: 0.7873\n",
      "Epoch 596/2000\n",
      "623/623 [==============================] - 0s 240us/step - loss: 0.4246 - acc: 0.8074 - val_loss: 0.4973 - val_acc: 0.7873\n",
      "Epoch 597/2000\n",
      "623/623 [==============================] - 0s 230us/step - loss: 0.4365 - acc: 0.7978 - val_loss: 0.5007 - val_acc: 0.7873\n",
      "Epoch 598/2000\n",
      "623/623 [==============================] - 0s 245us/step - loss: 0.4247 - acc: 0.8074 - val_loss: 0.5041 - val_acc: 0.7836\n",
      "Epoch 599/2000\n",
      "623/623 [==============================] - 0s 205us/step - loss: 0.4266 - acc: 0.7994 - val_loss: 0.5069 - val_acc: 0.7836\n",
      "Epoch 600/2000\n",
      "623/623 [==============================] - 0s 282us/step - loss: 0.4255 - acc: 0.8154 - val_loss: 0.5028 - val_acc: 0.7910\n",
      "Epoch 601/2000\n",
      "623/623 [==============================] - 0s 258us/step - loss: 0.4181 - acc: 0.8154 - val_loss: 0.5053 - val_acc: 0.7799\n",
      "Epoch 602/2000\n",
      "623/623 [==============================] - 0s 262us/step - loss: 0.4135 - acc: 0.8138 - val_loss: 0.5136 - val_acc: 0.7799\n",
      "Epoch 603/2000\n",
      "623/623 [==============================] - 0s 331us/step - loss: 0.4201 - acc: 0.8218 - val_loss: 0.5036 - val_acc: 0.7873\n",
      "Epoch 604/2000\n",
      "623/623 [==============================] - 0s 233us/step - loss: 0.4190 - acc: 0.8218 - val_loss: 0.5032 - val_acc: 0.7910\n",
      "Epoch 605/2000\n",
      "623/623 [==============================] - 0s 234us/step - loss: 0.4354 - acc: 0.8090 - val_loss: 0.5091 - val_acc: 0.7799\n",
      "Epoch 606/2000\n",
      "623/623 [==============================] - 0s 276us/step - loss: 0.4180 - acc: 0.8090 - val_loss: 0.5155 - val_acc: 0.7836\n",
      "Epoch 607/2000\n",
      "623/623 [==============================] - 0s 229us/step - loss: 0.4159 - acc: 0.8234 - val_loss: 0.5172 - val_acc: 0.7724\n",
      "Epoch 608/2000\n",
      "623/623 [==============================] - 0s 233us/step - loss: 0.4291 - acc: 0.8186 - val_loss: 0.5044 - val_acc: 0.7910\n",
      "Epoch 609/2000\n",
      "623/623 [==============================] - 0s 220us/step - loss: 0.4272 - acc: 0.8058 - val_loss: 0.5021 - val_acc: 0.7948\n",
      "Epoch 610/2000\n",
      "623/623 [==============================] - 0s 227us/step - loss: 0.4202 - acc: 0.8170 - val_loss: 0.5089 - val_acc: 0.7948\n",
      "Epoch 611/2000\n",
      "623/623 [==============================] - 0s 234us/step - loss: 0.4007 - acc: 0.8379 - val_loss: 0.5064 - val_acc: 0.7948\n",
      "Epoch 612/2000\n",
      "623/623 [==============================] - 0s 273us/step - loss: 0.4171 - acc: 0.8218 - val_loss: 0.5046 - val_acc: 0.7836\n",
      "Epoch 613/2000\n",
      "623/623 [==============================] - 0s 236us/step - loss: 0.4168 - acc: 0.8138 - val_loss: 0.5037 - val_acc: 0.7873\n",
      "Epoch 614/2000\n",
      "623/623 [==============================] - 0s 232us/step - loss: 0.4169 - acc: 0.8170 - val_loss: 0.5071 - val_acc: 0.7910\n",
      "Epoch 615/2000\n",
      "623/623 [==============================] - 0s 262us/step - loss: 0.4110 - acc: 0.8250 - val_loss: 0.5041 - val_acc: 0.7761\n",
      "Epoch 616/2000\n",
      "623/623 [==============================] - 0s 213us/step - loss: 0.4155 - acc: 0.8250 - val_loss: 0.5053 - val_acc: 0.7948\n",
      "Epoch 617/2000\n",
      "623/623 [==============================] - 0s 232us/step - loss: 0.4162 - acc: 0.8315 - val_loss: 0.5088 - val_acc: 0.7836\n",
      "Epoch 618/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4217 - acc: 0.8138 - val_loss: 0.5057 - val_acc: 0.7799\n",
      "Epoch 619/2000\n",
      "623/623 [==============================] - 0s 236us/step - loss: 0.4337 - acc: 0.8074 - val_loss: 0.5064 - val_acc: 0.7761\n",
      "Epoch 620/2000\n",
      "623/623 [==============================] - 0s 202us/step - loss: 0.4220 - acc: 0.8234 - val_loss: 0.5048 - val_acc: 0.7873\n",
      "Epoch 621/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.4271 - acc: 0.8010 - val_loss: 0.5000 - val_acc: 0.7873\n",
      "Epoch 622/2000\n",
      "623/623 [==============================] - 0s 254us/step - loss: 0.4152 - acc: 0.8202 - val_loss: 0.5052 - val_acc: 0.7761\n",
      "Epoch 623/2000\n",
      "623/623 [==============================] - 0s 261us/step - loss: 0.4203 - acc: 0.8170 - val_loss: 0.4992 - val_acc: 0.7948\n",
      "Epoch 624/2000\n",
      "623/623 [==============================] - 0s 228us/step - loss: 0.4054 - acc: 0.8170 - val_loss: 0.4999 - val_acc: 0.7761\n",
      "Epoch 625/2000\n",
      "623/623 [==============================] - 0s 238us/step - loss: 0.4163 - acc: 0.8106 - val_loss: 0.5043 - val_acc: 0.7761\n",
      "Epoch 626/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4157 - acc: 0.8122 - val_loss: 0.5099 - val_acc: 0.7687\n",
      "Epoch 627/2000\n",
      "623/623 [==============================] - 0s 259us/step - loss: 0.4256 - acc: 0.8234 - val_loss: 0.5048 - val_acc: 0.7761\n",
      "Epoch 628/2000\n",
      "623/623 [==============================] - 0s 232us/step - loss: 0.4087 - acc: 0.8250 - val_loss: 0.5059 - val_acc: 0.7836\n",
      "Epoch 629/2000\n",
      "623/623 [==============================] - 0s 206us/step - loss: 0.4358 - acc: 0.8042 - val_loss: 0.5026 - val_acc: 0.7799\n",
      "Epoch 630/2000\n",
      "623/623 [==============================] - 0s 234us/step - loss: 0.4069 - acc: 0.8266 - val_loss: 0.5051 - val_acc: 0.7836\n",
      "Epoch 631/2000\n",
      "623/623 [==============================] - 0s 218us/step - loss: 0.4095 - acc: 0.8218 - val_loss: 0.5041 - val_acc: 0.7836\n",
      "Epoch 632/2000\n",
      "623/623 [==============================] - 0s 223us/step - loss: 0.4097 - acc: 0.8218 - val_loss: 0.5057 - val_acc: 0.7836\n",
      "Epoch 633/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4107 - acc: 0.8202 - val_loss: 0.5107 - val_acc: 0.7873\n",
      "Epoch 634/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.4008 - acc: 0.8250 - val_loss: 0.5104 - val_acc: 0.7873\n",
      "Epoch 635/2000\n",
      "623/623 [==============================] - 0s 261us/step - loss: 0.4199 - acc: 0.8218 - val_loss: 0.5109 - val_acc: 0.7799\n",
      "Epoch 636/2000\n",
      "623/623 [==============================] - 0s 232us/step - loss: 0.4159 - acc: 0.8138 - val_loss: 0.5075 - val_acc: 0.7873\n",
      "Epoch 637/2000\n",
      "623/623 [==============================] - 0s 235us/step - loss: 0.4122 - acc: 0.8234 - val_loss: 0.5118 - val_acc: 0.7873\n",
      "Epoch 638/2000\n",
      "623/623 [==============================] - 0s 214us/step - loss: 0.4166 - acc: 0.8186 - val_loss: 0.5087 - val_acc: 0.7836\n",
      "Epoch 639/2000\n",
      "623/623 [==============================] - 0s 251us/step - loss: 0.4165 - acc: 0.8170 - val_loss: 0.5100 - val_acc: 0.7910\n",
      "Epoch 640/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.4115 - acc: 0.8138 - val_loss: 0.5161 - val_acc: 0.7724\n",
      "Epoch 641/2000\n",
      "623/623 [==============================] - 0s 221us/step - loss: 0.4214 - acc: 0.8186 - val_loss: 0.5057 - val_acc: 0.7836\n",
      "Epoch 642/2000\n",
      "623/623 [==============================] - 0s 261us/step - loss: 0.4137 - acc: 0.8186 - val_loss: 0.5071 - val_acc: 0.7761\n",
      "Epoch 643/2000\n",
      "623/623 [==============================] - 0s 221us/step - loss: 0.4184 - acc: 0.8106 - val_loss: 0.5077 - val_acc: 0.7873\n",
      "Epoch 644/2000\n",
      "623/623 [==============================] - 0s 252us/step - loss: 0.4160 - acc: 0.8250 - val_loss: 0.5068 - val_acc: 0.7799\n",
      "Epoch 645/2000\n",
      "623/623 [==============================] - 0s 257us/step - loss: 0.3995 - acc: 0.8234 - val_loss: 0.5131 - val_acc: 0.7873\n",
      "Epoch 646/2000\n",
      "623/623 [==============================] - 0s 317us/step - loss: 0.4170 - acc: 0.8138 - val_loss: 0.5065 - val_acc: 0.7910\n",
      "Epoch 647/2000\n",
      "623/623 [==============================] - 0s 284us/step - loss: 0.4182 - acc: 0.8122 - val_loss: 0.5123 - val_acc: 0.7910\n",
      "Epoch 648/2000\n",
      "623/623 [==============================] - 0s 268us/step - loss: 0.4149 - acc: 0.8202 - val_loss: 0.5036 - val_acc: 0.7836\n",
      "Epoch 649/2000\n",
      "623/623 [==============================] - 0s 247us/step - loss: 0.4257 - acc: 0.8154 - val_loss: 0.5036 - val_acc: 0.7836\n",
      "Epoch 650/2000\n",
      "623/623 [==============================] - 0s 211us/step - loss: 0.4112 - acc: 0.8283 - val_loss: 0.5074 - val_acc: 0.7948\n",
      "Epoch 651/2000\n",
      "623/623 [==============================] - 0s 220us/step - loss: 0.4195 - acc: 0.8202 - val_loss: 0.5132 - val_acc: 0.7687\n",
      "Epoch 652/2000\n",
      "623/623 [==============================] - 0s 204us/step - loss: 0.4160 - acc: 0.8202 - val_loss: 0.5095 - val_acc: 0.7873\n",
      "Epoch 653/2000\n",
      "623/623 [==============================] - 0s 244us/step - loss: 0.4178 - acc: 0.8202 - val_loss: 0.5073 - val_acc: 0.7873\n",
      "Epoch 654/2000\n",
      "623/623 [==============================] - 0s 204us/step - loss: 0.4160 - acc: 0.8218 - val_loss: 0.5094 - val_acc: 0.7873\n",
      "Epoch 655/2000\n",
      "623/623 [==============================] - 0s 201us/step - loss: 0.4028 - acc: 0.8315 - val_loss: 0.5155 - val_acc: 0.7873\n",
      "Epoch 656/2000\n",
      "623/623 [==============================] - 0s 205us/step - loss: 0.4068 - acc: 0.8347 - val_loss: 0.5133 - val_acc: 0.7836\n",
      "Epoch 657/2000\n",
      "623/623 [==============================] - 0s 226us/step - loss: 0.4250 - acc: 0.8202 - val_loss: 0.5127 - val_acc: 0.7836\n",
      "Epoch 658/2000\n",
      "623/623 [==============================] - 0s 197us/step - loss: 0.4291 - acc: 0.8122 - val_loss: 0.5107 - val_acc: 0.7873\n",
      "Epoch 659/2000\n",
      "623/623 [==============================] - 0s 235us/step - loss: 0.4183 - acc: 0.8154 - val_loss: 0.5116 - val_acc: 0.7873\n",
      "Epoch 660/2000\n",
      "623/623 [==============================] - 0s 222us/step - loss: 0.4106 - acc: 0.8234 - val_loss: 0.5199 - val_acc: 0.7799\n",
      "Epoch 661/2000\n",
      "623/623 [==============================] - 0s 216us/step - loss: 0.4087 - acc: 0.8218 - val_loss: 0.5089 - val_acc: 0.7761\n",
      "Epoch 662/2000\n",
      "623/623 [==============================] - 0s 221us/step - loss: 0.4024 - acc: 0.8299 - val_loss: 0.5076 - val_acc: 0.7836\n",
      "Epoch 663/2000\n",
      "623/623 [==============================] - 0s 198us/step - loss: 0.4089 - acc: 0.8266 - val_loss: 0.5158 - val_acc: 0.7836\n",
      "Epoch 664/2000\n",
      "623/623 [==============================] - 0s 233us/step - loss: 0.4130 - acc: 0.8202 - val_loss: 0.5094 - val_acc: 0.7799\n",
      "Epoch 665/2000\n",
      "623/623 [==============================] - 0s 229us/step - loss: 0.4262 - acc: 0.8234 - val_loss: 0.5119 - val_acc: 0.7761\n",
      "Epoch 666/2000\n",
      "623/623 [==============================] - 0s 207us/step - loss: 0.4254 - acc: 0.8042 - val_loss: 0.5117 - val_acc: 0.7985\n",
      "Epoch 667/2000\n",
      "623/623 [==============================] - 0s 233us/step - loss: 0.4177 - acc: 0.8186 - val_loss: 0.5116 - val_acc: 0.7910\n",
      "Epoch 668/2000\n",
      "623/623 [==============================] - 0s 202us/step - loss: 0.4121 - acc: 0.8234 - val_loss: 0.5157 - val_acc: 0.7799\n",
      "Epoch 669/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.4099 - acc: 0.8186 - val_loss: 0.5119 - val_acc: 0.7836\n",
      "Epoch 670/2000\n",
      "623/623 [==============================] - 0s 226us/step - loss: 0.4099 - acc: 0.8250 - val_loss: 0.5059 - val_acc: 0.7873\n",
      "Epoch 671/2000\n",
      "623/623 [==============================] - 0s 199us/step - loss: 0.4063 - acc: 0.8154 - val_loss: 0.5089 - val_acc: 0.7873\n",
      "Epoch 672/2000\n",
      "623/623 [==============================] - 0s 214us/step - loss: 0.4116 - acc: 0.8250 - val_loss: 0.5087 - val_acc: 0.7836\n",
      "Epoch 673/2000\n",
      "623/623 [==============================] - 0s 204us/step - loss: 0.4050 - acc: 0.8250 - val_loss: 0.5089 - val_acc: 0.7873\n",
      "Epoch 674/2000\n",
      "623/623 [==============================] - 0s 250us/step - loss: 0.4025 - acc: 0.8315 - val_loss: 0.5160 - val_acc: 0.7873\n",
      "Epoch 675/2000\n",
      "623/623 [==============================] - 0s 199us/step - loss: 0.4172 - acc: 0.8122 - val_loss: 0.5108 - val_acc: 0.7761\n",
      "Epoch 676/2000\n",
      "623/623 [==============================] - 0s 260us/step - loss: 0.4196 - acc: 0.8250 - val_loss: 0.5103 - val_acc: 0.7687\n",
      "Epoch 677/2000\n",
      "623/623 [==============================] - 0s 236us/step - loss: 0.3938 - acc: 0.8363 - val_loss: 0.5144 - val_acc: 0.7873\n",
      "Epoch 678/2000\n",
      "623/623 [==============================] - 0s 237us/step - loss: 0.4141 - acc: 0.8218 - val_loss: 0.5175 - val_acc: 0.7836\n",
      "Epoch 679/2000\n",
      "623/623 [==============================] - 0s 237us/step - loss: 0.3945 - acc: 0.8459 - val_loss: 0.5128 - val_acc: 0.7836\n",
      "Epoch 680/2000\n",
      "623/623 [==============================] - 0s 265us/step - loss: 0.4083 - acc: 0.8234 - val_loss: 0.5225 - val_acc: 0.7836\n",
      "Epoch 681/2000\n",
      "623/623 [==============================] - 0s 270us/step - loss: 0.4156 - acc: 0.8347 - val_loss: 0.5142 - val_acc: 0.7910\n",
      "Epoch 682/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.4026 - acc: 0.8234 - val_loss: 0.5173 - val_acc: 0.7948\n",
      "Epoch 683/2000\n",
      "623/623 [==============================] - 0s 235us/step - loss: 0.4175 - acc: 0.8202 - val_loss: 0.5104 - val_acc: 0.7836\n",
      "Epoch 684/2000\n",
      "623/623 [==============================] - 0s 212us/step - loss: 0.4029 - acc: 0.8186 - val_loss: 0.5135 - val_acc: 0.7910\n",
      "Epoch 685/2000\n",
      "623/623 [==============================] - 0s 253us/step - loss: 0.3989 - acc: 0.8315 - val_loss: 0.5300 - val_acc: 0.7799\n",
      "Epoch 686/2000\n",
      "623/623 [==============================] - 0s 219us/step - loss: 0.4100 - acc: 0.8299 - val_loss: 0.5239 - val_acc: 0.7873\n",
      "Epoch 687/2000\n",
      "623/623 [==============================] - 0s 235us/step - loss: 0.4083 - acc: 0.8202 - val_loss: 0.5179 - val_acc: 0.7761\n",
      "Epoch 688/2000\n",
      "623/623 [==============================] - 0s 202us/step - loss: 0.4183 - acc: 0.8138 - val_loss: 0.5158 - val_acc: 0.7836\n",
      "Epoch 689/2000\n",
      "623/623 [==============================] - 0s 199us/step - loss: 0.4069 - acc: 0.8122 - val_loss: 0.5209 - val_acc: 0.7910\n",
      "Epoch 690/2000\n",
      "623/623 [==============================] - 0s 201us/step - loss: 0.3889 - acc: 0.8283 - val_loss: 0.5365 - val_acc: 0.7873\n",
      "Epoch 691/2000\n",
      "623/623 [==============================] - 0s 230us/step - loss: 0.3931 - acc: 0.8299 - val_loss: 0.5218 - val_acc: 0.7836\n",
      "Epoch 692/2000\n",
      "623/623 [==============================] - 0s 228us/step - loss: 0.4187 - acc: 0.8170 - val_loss: 0.5224 - val_acc: 0.7836\n",
      "Epoch 693/2000\n",
      "623/623 [==============================] - 0s 239us/step - loss: 0.3971 - acc: 0.8218 - val_loss: 0.5359 - val_acc: 0.7799\n",
      "Epoch 694/2000\n",
      "623/623 [==============================] - 0s 201us/step - loss: 0.4143 - acc: 0.8170 - val_loss: 0.5265 - val_acc: 0.7910\n",
      "Epoch 695/2000\n",
      "623/623 [==============================] - 0s 232us/step - loss: 0.4059 - acc: 0.8234 - val_loss: 0.5153 - val_acc: 0.7836\n",
      "Epoch 696/2000\n",
      "623/623 [==============================] - 0s 220us/step - loss: 0.4102 - acc: 0.8154 - val_loss: 0.5135 - val_acc: 0.7873\n",
      "Epoch 697/2000\n",
      "623/623 [==============================] - 0s 222us/step - loss: 0.4084 - acc: 0.8202 - val_loss: 0.5120 - val_acc: 0.7799\n",
      "Epoch 698/2000\n",
      "623/623 [==============================] - 0s 205us/step - loss: 0.4020 - acc: 0.8331 - val_loss: 0.5265 - val_acc: 0.7836\n",
      "Epoch 699/2000\n",
      "623/623 [==============================] - 0s 226us/step - loss: 0.4123 - acc: 0.8331 - val_loss: 0.5131 - val_acc: 0.7910\n",
      "Epoch 700/2000\n",
      "623/623 [==============================] - 0s 215us/step - loss: 0.3938 - acc: 0.8250 - val_loss: 0.5188 - val_acc: 0.7873\n",
      "Epoch 701/2000\n",
      "623/623 [==============================] - 0s 210us/step - loss: 0.4107 - acc: 0.8299 - val_loss: 0.5168 - val_acc: 0.7873\n",
      "Epoch 702/2000\n",
      "623/623 [==============================] - 0s 272us/step - loss: 0.4075 - acc: 0.8154 - val_loss: 0.5161 - val_acc: 0.7873\n",
      "Epoch 703/2000\n",
      "623/623 [==============================] - 0s 228us/step - loss: 0.4012 - acc: 0.8315 - val_loss: 0.5185 - val_acc: 0.7799\n",
      "Epoch 704/2000\n",
      "623/623 [==============================] - 0s 223us/step - loss: 0.4092 - acc: 0.8234 - val_loss: 0.5224 - val_acc: 0.7836\n",
      "Epoch 705/2000\n",
      "623/623 [==============================] - 0s 238us/step - loss: 0.4194 - acc: 0.8122 - val_loss: 0.5184 - val_acc: 0.7799\n",
      "Epoch 706/2000\n",
      "623/623 [==============================] - 0s 208us/step - loss: 0.4098 - acc: 0.8186 - val_loss: 0.5148 - val_acc: 0.7873\n",
      "Epoch 707/2000\n",
      "623/623 [==============================] - 0s 235us/step - loss: 0.4125 - acc: 0.8170 - val_loss: 0.5120 - val_acc: 0.7836\n",
      "Epoch 708/2000\n",
      "623/623 [==============================] - 0s 225us/step - loss: 0.4081 - acc: 0.8266 - val_loss: 0.5134 - val_acc: 0.7799\n",
      "Epoch 709/2000\n",
      "623/623 [==============================] - 0s 200us/step - loss: 0.3938 - acc: 0.8379 - val_loss: 0.5183 - val_acc: 0.7836\n",
      "Epoch 710/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623/623 [==============================] - 0s 221us/step - loss: 0.4002 - acc: 0.8250 - val_loss: 0.5282 - val_acc: 0.7873\n",
      "Epoch 711/2000\n",
      "623/623 [==============================] - 0s 231us/step - loss: 0.3952 - acc: 0.8315 - val_loss: 0.5222 - val_acc: 0.7873\n",
      "Epoch 712/2000\n",
      "623/623 [==============================] - 0s 230us/step - loss: 0.4010 - acc: 0.8202 - val_loss: 0.5274 - val_acc: 0.7873\n",
      "Epoch 713/2000\n",
      "623/623 [==============================] - 0s 254us/step - loss: 0.4082 - acc: 0.8138 - val_loss: 0.5205 - val_acc: 0.7836\n",
      "Epoch 714/2000\n",
      "623/623 [==============================] - 0s 240us/step - loss: 0.4060 - acc: 0.8186 - val_loss: 0.5175 - val_acc: 0.7761\n",
      "Epoch 715/2000\n",
      "623/623 [==============================] - 0s 216us/step - loss: 0.4054 - acc: 0.8186 - val_loss: 0.5266 - val_acc: 0.7873\n",
      "Epoch 716/2000\n",
      "623/623 [==============================] - 0s 219us/step - loss: 0.4011 - acc: 0.8299 - val_loss: 0.5187 - val_acc: 0.7836\n",
      "Epoch 717/2000\n",
      "623/623 [==============================] - 0s 239us/step - loss: 0.4107 - acc: 0.8154 - val_loss: 0.5206 - val_acc: 0.7799\n",
      "Epoch 718/2000\n",
      "623/623 [==============================] - 0s 229us/step - loss: 0.4076 - acc: 0.8218 - val_loss: 0.5282 - val_acc: 0.7910\n",
      "Epoch 719/2000\n",
      "623/623 [==============================] - 0s 210us/step - loss: 0.4159 - acc: 0.8218 - val_loss: 0.5360 - val_acc: 0.7761\n",
      "Epoch 720/2000\n",
      "623/623 [==============================] - 0s 211us/step - loss: 0.4071 - acc: 0.8154 - val_loss: 0.5296 - val_acc: 0.7836\n",
      "Epoch 721/2000\n",
      "623/623 [==============================] - 0s 212us/step - loss: 0.4051 - acc: 0.8283 - val_loss: 0.5227 - val_acc: 0.7761\n",
      "Epoch 722/2000\n",
      "623/623 [==============================] - 0s 236us/step - loss: 0.4107 - acc: 0.8202 - val_loss: 0.5277 - val_acc: 0.7873\n",
      "Epoch 723/2000\n",
      "623/623 [==============================] - 0s 206us/step - loss: 0.4133 - acc: 0.8218 - val_loss: 0.5212 - val_acc: 0.7799\n",
      "Epoch 724/2000\n",
      "623/623 [==============================] - 0s 224us/step - loss: 0.3882 - acc: 0.8347 - val_loss: 0.5270 - val_acc: 0.7873\n",
      "Epoch 725/2000\n",
      "623/623 [==============================] - 0s 229us/step - loss: 0.4048 - acc: 0.8347 - val_loss: 0.5210 - val_acc: 0.7836\n",
      "Epoch 726/2000\n",
      "623/623 [==============================] - 0s 234us/step - loss: 0.3972 - acc: 0.8315 - val_loss: 0.5247 - val_acc: 0.7761\n",
      "Epoch 727/2000\n",
      "623/623 [==============================] - 0s 239us/step - loss: 0.4052 - acc: 0.8266 - val_loss: 0.5204 - val_acc: 0.7836\n",
      "Epoch 728/2000\n",
      "623/623 [==============================] - 0s 226us/step - loss: 0.3959 - acc: 0.8234 - val_loss: 0.5239 - val_acc: 0.7799\n",
      "Epoch 729/2000\n",
      "623/623 [==============================] - 0s 241us/step - loss: 0.3908 - acc: 0.8283 - val_loss: 0.5229 - val_acc: 0.7836\n",
      "Epoch 730/2000\n",
      "623/623 [==============================] - 0s 228us/step - loss: 0.4066 - acc: 0.8299 - val_loss: 0.5207 - val_acc: 0.7948\n",
      "Epoch 731/2000\n",
      "623/623 [==============================] - 0s 242us/step - loss: 0.4170 - acc: 0.8234 - val_loss: 0.5206 - val_acc: 0.7910\n",
      "Epoch 732/2000\n",
      "623/623 [==============================] - 0s 243us/step - loss: 0.4130 - acc: 0.8154 - val_loss: 0.5263 - val_acc: 0.7799\n",
      "Epoch 733/2000\n",
      "623/623 [==============================] - 0s 222us/step - loss: 0.3853 - acc: 0.8395 - val_loss: 0.5276 - val_acc: 0.7761\n",
      "Epoch 734/2000\n",
      "623/623 [==============================] - 0s 246us/step - loss: 0.4057 - acc: 0.8266 - val_loss: 0.5268 - val_acc: 0.7761\n",
      "Epoch 735/2000\n",
      "623/623 [==============================] - 0s 234us/step - loss: 0.4078 - acc: 0.8250 - val_loss: 0.5201 - val_acc: 0.7836\n",
      "Epoch 736/2000\n",
      "623/623 [==============================] - 0s 245us/step - loss: 0.4011 - acc: 0.8218 - val_loss: 0.5239 - val_acc: 0.7761\n",
      "Epoch 737/2000\n",
      "623/623 [==============================] - ETA: 0s - loss: 0.3959 - acc: 0.833 - 0s 242us/step - loss: 0.3935 - acc: 0.8299 - val_loss: 0.5274 - val_acc: 0.7836\n",
      "Epoch 738/2000\n",
      "623/623 [==============================] - 0s 230us/step - loss: 0.4059 - acc: 0.8283 - val_loss: 0.5246 - val_acc: 0.7761\n",
      "Epoch 739/2000\n",
      "623/623 [==============================] - 0s 213us/step - loss: 0.3910 - acc: 0.8266 - val_loss: 0.5327 - val_acc: 0.7873\n",
      "Epoch 740/2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b440677d0a8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           validation_split=0.3)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# -------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/envs/sci/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/envs/sci/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/envs/sci/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2665\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2667\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/envs/sci/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2649\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/envs/sci/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/envs/sci/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/envs/sci/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/envs/sci/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/envs/sci/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------- Fitting the model -------------------\n",
    "model.compile(keras.optimizers.Adagrad(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, Y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_split=0.3)\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
