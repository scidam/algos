{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining with Titanic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import fcluster, single, complete\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform \n",
    "from collections.abc import Iterable\n",
    "from mlpipes.pfunc import get_ohe\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from mlpipes.pfunc import *\n",
    "\n",
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "test = pd.read_csv(\"./data/test.csv\")\n",
    "combined = pd.concat([train, test], sort=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some preprocessing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_name(s): \n",
    "    a, b = s.split(',')\n",
    "    family_name = a.strip()\n",
    "    title = b.split('.')[0].strip()\n",
    "    first_name = b.split('.')[1].split()[0].strip()\n",
    "    return (first_name.replace('(', '').replace(')', ''), title, family_name)\n",
    "\n",
    "def parse_cabin_letter(column):\n",
    "    letter_pat = re.compile('([A-Za-z])\\d+')\n",
    "    return list(map(lambda x: letter_pat.findall(str(x))[0] if letter_pat.findall(str(x)) else pd.np.nan, column.values.tolist()))\n",
    "\n",
    "def parse_ticket_number(column):\n",
    "    number_pat = re.compile('\\d{3,}')\n",
    "    numbers = map(lambda x: number_pat.findall(x)[0] if number_pat.findall(x) else pd.np.nan, column)\n",
    "    return pd.Series(numbers)\n",
    "\n",
    "\n",
    "def get_friendship_group(df):\n",
    "    friendship_group_counter = 0\n",
    "    if 'family_name' not in df.columns:\n",
    "        family_names = pd.Series(map(lambda x: parse_name(x)[-1], df.Name))\n",
    "    else:\n",
    "        family_names = df.family_name\n",
    "    cabins = pd.Series(map(parse_cabin_letter, df.Cabin))\n",
    "    ticket_grouping = []\n",
    "    for family, count in family_names.value_counts().items():\n",
    "        family_mask = family_names == family\n",
    "        \n",
    "        if count == 1:\n",
    "            ticket_grouping.append(friendship_group_counter)\n",
    "            friendship_group_counter += 1\n",
    "            continue\n",
    "\n",
    "            \n",
    "def get_family_name(df):\n",
    "    family_names = pd.Series(map(lambda x: parse_name(x)[-1], df.Name))\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:,'family_name'] = family_names\n",
    "    return df_\n",
    "            \n",
    "def get_ticket_group(df):\n",
    "    df_ = df.copy()\n",
    "    grouped = df.Ticket.groupby(parse_ticket_number(df.Ticket))\n",
    "    for ind, key in enumerate(grouped.indices):\n",
    "        df_.loc[grouped.indices[key], 'ticket_group'] = ind if len(grouped.indices[key])>1 else -1\n",
    "    return df_\n",
    "\n",
    "def get_cabin_letter(df):\n",
    "    df_ = df.copy()\n",
    "    cabins = parse_cabin_letter(df.Cabin)\n",
    "    df_.loc[:, 'cabin_na'] = pd.isnull(df.Cabin)\n",
    "    df_.loc[:, 'cabin'] = cabins\n",
    "    return df_\n",
    "\n",
    "def get_is_alone(df):\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, 'is_alone'] = (df.loc[:, 'Parch'] + df.loc[:, 'SibSp'] + 1 == 1).astype(int)\n",
    "    return df_\n",
    "\n",
    "def get_family_size(df):\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, 'family_size'] = df_.loc[:, 'Parch'] + df_.loc[:, 'SibSp'] + 1\n",
    "    return df_\n",
    "\n",
    "\n",
    "def get_titles(df):\n",
    "    df_ = df.copy()\n",
    "    titles = pd.Series(map(lambda x: parse_name(x)[1], df.Name))\n",
    "    df_.loc[:, 'title'] = titles\n",
    "    return df_\n",
    "\n",
    "def discretize_faries(df, ngroups=3):\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, 'fares'] = pd.cut(df_.loc[:,'Fare'], ngroups, labels=False)\n",
    "    return df_\n",
    "\n",
    "def estimate_age(df):\n",
    "    estimates = []\n",
    "    for ind, row in df.loc[df.Age.isnull(), :].iterrows():\n",
    "        # NOTE: Could be rewritten using vectorized notation\n",
    "        if row.title in ['Master', 'Mr', 'Miss', 'Rev', 'Dr']:\n",
    "            estimates.append(df.groupby(['title', 'Sex']).median().loc[[row.title, row.Sex],'Age'].values[0])\n",
    "        else:\n",
    "            estimates.append(df.groupby(['Sex', 'Pclass']).median().loc[[row.Sex], 'Age'].values[0])\n",
    "    df_ = df.copy()\n",
    "    df_.loc[df.Age.isnull(), 'Age'] = estimates\n",
    "    return df_\n",
    "\n",
    "\n",
    "def get_cabin_groups(df):\n",
    "    num_pat = re.compile('\\d+')\n",
    "    let_pat = re.compile('[a-zA-Z]')\n",
    "    LONG_DISTANCE = 5\n",
    "    MEDIUM_DISTANCE = 4\n",
    "    NORMAL_DISTANCE = 3\n",
    "    SMALL_DISTANCE = 2\n",
    "    LOW_DISTANCE = 1\n",
    "    EQUAL = 0\n",
    "    def cabin_distance(u, v):\n",
    "        _u, _v = u[0], v[0]\n",
    "        if not isinstance(_u, Iterable) or not isinstance(_v, Iterable):\n",
    "            return LONG_DISTANCE\n",
    "        unums = list(map(int, sum(map(num_pat.findall, _u), [])))\n",
    "        vnums = list(map(int, sum(map(num_pat.findall, _v), [])))\n",
    "        ulets = list(sum(map(let_pat.findall, _u), []))\n",
    "        vlets = list(sum(map(let_pat.findall, _v), []))\n",
    "        if not(unums and vnums):\n",
    "            if set(ulets).intersection(vlets):\n",
    "                return EQUAL\n",
    "            else:\n",
    "                return MEDIUM_DISTANCE\n",
    "        if u == v:\n",
    "            return EQUAL\n",
    "        if set(_u).intersection(set(_v)):\n",
    "            return LOW_DISTANCE\n",
    "        if not set(ulets).intersection(set(vlets)):\n",
    "            return MEDIUM_DISTANCE\n",
    "        else:\n",
    "            for p in _u:\n",
    "                for q in _v:\n",
    "                    try:\n",
    "                        pval = list(map(int, num_pat.findall(p)))[0]\n",
    "                        qval = list(map(int, num_pat.findall(q)))[0]\n",
    "                        if p[0] == q[0] and (abs(pval - qval) <= 2):\n",
    "                            return SMALL_DISTANCE\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "            return NORMAL_DISTANCE\n",
    "        return MEDIUM_DISTANCE\n",
    "    distances = pdist(df.Cabin.apply(lambda x: x.split() if not isinstance(x, float) else x).values[:, np.newaxis], cabin_distance)\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, 'cabin_group'] = fcluster(complete(distances), SMALL_DISTANCE, criterion='distance')\n",
    "    df_.cabin_group = df_.groupby('cabin_group')['cabin_group'].transform(lambda x: x if len(x)>1 else pd.Series([-1]*len(x)))\n",
    "    return df_\n",
    "\n",
    "\n",
    "def combine_titles(df):\n",
    "    df_ = df.copy()\n",
    "    df_['title'] = df_['title'].replace(['Mlle'], 'Miss')\n",
    "    df_['title'] = df_['title'].replace(['Ms'], 'Miss')\n",
    "    df_['title'] = df_['title'].replace(['Mme'], 'Mrs')\n",
    "    df_['title'] = df_['title'].replace(['Lady', 'the Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'rare')\n",
    "    return df_\n",
    "\n",
    "\n",
    "def simple_encoder(df):\n",
    "    df_ = df.copy()\n",
    "    sex_mapping = {'male': 0, 'female':1}\n",
    "    embarked_mapping = {'S':0, 'Q':1, 'S':2}\n",
    "    df_.Embarked = df_.Embarked.map(embarked_mapping)\n",
    "    df_.Sex = df_.Sex.map(sex_mapping)\n",
    "    return df_\n",
    "\n",
    "def label_encode(df, **kwargs):\n",
    "    return get_le(df, **kwargs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = (('add_groups', get_ticket_group, {}),\n",
    "                          ('add_family_size', get_family_size, {}),\n",
    "                          ('add_titles', get_titles, {}),\n",
    "                          ('convert_fares', discretize_faries, {'ngroups': 4}),\n",
    "                          ('add_ticket_group', get_ticket_group, {}),\n",
    "                          ('fill_embarked', fill_na_simple, {'colnames': ('Embarked',),\n",
    "                                                             'methods': (lambda x: pd.Series(x).mode()[0],)}),\n",
    "                          ('add_family_name', get_family_name, {}),\n",
    "                          ('add_ages', estimate_age, {}),\n",
    "                          ('add_cabin_groups', get_cabin_groups, {}),\n",
    "                          ('combine_titles', combine_titles, {}),\n",
    "                          ('sex_encoder', simple_encoder, {}),\n",
    "                          ('drop_columns', drop_columns, {'colnames': ('Survived',\n",
    "                                                                       'PassengerId',\n",
    "                                                                       'SibSp',\n",
    "                                                                       'Parch',\n",
    "                                                                       'Ticket',\n",
    "                                                                       'Fare',\n",
    "                                                                       'family_name',\n",
    "                                                                       'Name',\n",
    "                                                                       'Cabin',\n",
    "                                                                       'Sex',\n",
    "                                                                       )}),\n",
    "                          ('get_le', label_encode, {'colnames': ('ticket_group', 'cabin_group',\n",
    "                                                         'title', 'fares', 'Embarked')})\n",
    "                         )\n",
    "\n",
    "def process(pipeline, data):\n",
    "    data_ = data.copy()\n",
    "    for name, func, kwargs in pipeline:\n",
    "        print(\"=========== Step: %s ===========\" % name)\n",
    "        data_ = func(data_, **kwargs)\n",
    "        print(\"=\" * 40)\n",
    "    return data_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing steps (feature engeneering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Step: add_groups ===========\n",
      "========================================\n",
      "=========== Step: add_family_size ===========\n",
      "========================================\n",
      "=========== Step: add_titles ===========\n",
      "========================================\n",
      "=========== Step: convert_fares ===========\n",
      "========================================\n",
      "=========== Step: add_ticket_group ===========\n",
      "========================================\n",
      "=========== Step: fill_embarked ===========\n",
      "========================================\n",
      "=========== Step: add_family_name ===========\n",
      "========================================\n",
      "=========== Step: add_ages ===========\n",
      "========================================\n",
      "=========== Step: add_cabin_groups ===========\n",
      "========================================\n",
      "=========== Step: combine_titles ===========\n",
      "========================================\n",
      "=========== Step: sex_encoder ===========\n",
      "========================================\n",
      "=========== Step: drop_columns ===========\n",
      "========================================\n",
      "=========== Step: get_le ===========\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "processed  = process(preprocessing_pipeline, combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2000 candidates, totalling 6000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 202 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 452 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done 802 tasks      | elapsed:   52.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1252 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1802 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2452 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3202 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4052 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5002 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6000 out of 6000 | elapsed:  6.4min finished\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "X = processed.iloc[:train.shape[0]].values\n",
    "parameters_RF = {'n_estimators': [20,60,80,100,120,200,300,800],\n",
    "              'max_depth': (None, 2, 3, 5, 7, 10, 15),\n",
    "              'criterion': ('gini', 'entropy'),\n",
    "              'max_features': ('auto', 'log2', None),\n",
    "              'oob_score': (True, False)\n",
    "             }\n",
    "\n",
    "parameters_GB = {'n_estimators': randint(20, 300),\n",
    "                 'max_depth': randint(3, 20),\n",
    "                 'subsample': uniform(0.7, 0.3),\n",
    "                 'learning_rate': uniform(0.0001, 0.3),\n",
    "                 'max_features': ('auto', 'log2', None),\n",
    "                 'min_samples_leaf': randint(3, 10),\n",
    "                 'min_samples_split' : randint(2, 10)\n",
    "                }\n",
    "\n",
    "y = train.Survived.values\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    clfcv = RandomizedSearchCV(clf, param_distributions=parameters_GB, n_iter=6000, scoring='f1',\n",
    "                               cv=3, verbose=1, n_jobs=-1)\n",
    "    clfcv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74242424, 0.74820144, 0.79699248, 0.77310924, 0.82014388])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clfcv.best_estimator_, X, y, cv=5, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7669155113596775"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({'PassengerId':test.PassengerId, 'Survived': clfcv.best_estimator_.predict(processed.iloc[train.shape[0]:].values)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
