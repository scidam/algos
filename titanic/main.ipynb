{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining with Titanic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import fcluster, single, complete\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform \n",
    "from collections.abc import Iterable\n",
    "from mlpipes.pfunc import get_ohe\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from mlpipes.pfunc import *\n",
    "\n",
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "test = pd.read_csv(\"./data/test.csv\")\n",
    "combined = pd.concat([train, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some preprocessing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_name(s): \n",
    "    a, b = s.split(',')\n",
    "    family_name = a.strip()\n",
    "    title = b.split('.')[0].strip()\n",
    "    first_name = b.split('.')[1].split()[0].strip()\n",
    "    return (first_name.replace('(', '').replace(')', ''), title, family_name)\n",
    "\n",
    "def parse_cabin_letter(column):\n",
    "    letter_pat = re.compile('([A-Za-z])\\d+')\n",
    "    return list(map(lambda x: letter_pat.findall(str(x))[0] if letter_pat.findall(str(x)) else pd.np.nan, column.values.tolist()))\n",
    "\n",
    "def parse_ticket_number(column):\n",
    "    number_pat = re.compile('\\d{3,}')\n",
    "    numbers = map(lambda x: number_pat.findall(x)[0] if number_pat.findall(x) else pd.np.nan, column)\n",
    "    return pd.Series(numbers)\n",
    "\n",
    "\n",
    "def get_friendship_group(df):\n",
    "    friendship_group_counter = 0\n",
    "    if 'family_name' not in df.columns:\n",
    "        family_names = pd.Series(map(lambda x: parse_name(x)[-1], df.Name))\n",
    "    else:\n",
    "        family_names = df.family_name\n",
    "    cabins = pd.Series(map(parse_cabin_letter, df.Cabin))\n",
    "    ticket_grouping = []\n",
    "    for family, count in family_names.value_counts().items():\n",
    "        family_mask = family_names == family\n",
    "        \n",
    "        if count == 1:\n",
    "            ticket_grouping.append(friendship_group_counter)\n",
    "            friendship_group_counter += 1\n",
    "            continue\n",
    "\n",
    "\n",
    "def get_family_name(df):\n",
    "    family_names = pd.Series(map(lambda x: parse_name(x)[-1], df.Name))\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:,'family_name'] = family_names\n",
    "    return df_\n",
    "            \n",
    "def get_ticket_group(df):\n",
    "    df_ = df.copy()\n",
    "    grouped = df.Ticket.groupby(parse_ticket_number(df.Ticket))\n",
    "    for ind, key in enumerate(grouped.indices):\n",
    "        df_.loc[grouped.indices[key], 'ticket_group'] = ind if len(grouped.indices[key])>1 else -1\n",
    "    return df_\n",
    "\n",
    "def get_cabin_letter(df):\n",
    "    df_ = df.copy()\n",
    "    cabins = parse_cabin_letter(df.Cabin)\n",
    "    df_.loc[:, 'cabin_na'] = pd.isnull(df.Cabin)\n",
    "    df_.loc[:, 'cabin'] = cabins\n",
    "    return df_\n",
    "\n",
    "def get_is_alone(df):\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, 'is_alone'] = (df.loc[:, 'Parch'] + df.loc[:, 'SibSp'] + 1 == 1).astype(int)\n",
    "    return df_\n",
    "\n",
    "def get_family_size(df):\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, 'family_size'] = df_.loc[:, 'Parch'] + df_.loc[:, 'SibSp'] + 1\n",
    "    return df_\n",
    "\n",
    "\n",
    "def get_titles(df):\n",
    "    df_ = df.copy()\n",
    "    titles = pd.Series(map(lambda x: parse_name(x)[1], df.Name))\n",
    "    df_.loc[:, 'title'] = titles\n",
    "    return df_\n",
    "\n",
    "def discretize_faries(df, ngroups=3):\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, 'fares'] = pd.cut(df_.loc[:,'Fare'], ngroups, labels=False)\n",
    "    return df_\n",
    "\n",
    "def discretize_ages(df, ngroups=3):\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, 'Age'] = pd.cut(df_.loc[:,'Age'], ngroups, labels=False)\n",
    "    return df_\n",
    "\n",
    "def estimate_age(df):\n",
    "    estimates = []\n",
    "    for ind, row in df.loc[df.Age.isnull(), :].iterrows():\n",
    "        # NOTE: Could be rewritten using vectorized notation\n",
    "        if row.title in ['Master', 'Mr', 'Miss', 'Rev', 'Dr']:\n",
    "            estimates.append(df.groupby(['title', 'Sex']).median().loc[[row.title, row.Sex],'Age'].values[0])\n",
    "        else:\n",
    "            estimates.append(df.groupby(['Sex', 'Pclass']).median().loc[[row.Sex], 'Age'].values[0])\n",
    "    df_ = df.copy()\n",
    "    df_.loc[df.Age.isnull(), 'Age'] = estimates\n",
    "    return df_\n",
    "\n",
    "\n",
    "def get_cabin_groups(df):\n",
    "    num_pat = re.compile('\\d+')\n",
    "    let_pat = re.compile('[a-zA-Z]')\n",
    "    LONG_DISTANCE = 5\n",
    "    MEDIUM_DISTANCE = 4\n",
    "    NORMAL_DISTANCE = 3\n",
    "    SMALL_DISTANCE = 2\n",
    "    LOW_DISTANCE = 1\n",
    "    EQUAL = 0\n",
    "    def cabin_distance(u, v):\n",
    "        _u, _v = u[0], v[0]\n",
    "        if not isinstance(_u, Iterable) or not isinstance(_v, Iterable):\n",
    "            return LONG_DISTANCE\n",
    "        unums = list(map(int, sum(map(num_pat.findall, _u), [])))\n",
    "        vnums = list(map(int, sum(map(num_pat.findall, _v), [])))\n",
    "        ulets = list(sum(map(let_pat.findall, _u), []))\n",
    "        vlets = list(sum(map(let_pat.findall, _v), []))\n",
    "        if not(unums and vnums):\n",
    "            if set(ulets).intersection(vlets):\n",
    "                return EQUAL\n",
    "            else:\n",
    "                return MEDIUM_DISTANCE\n",
    "        if u == v:\n",
    "            return EQUAL\n",
    "        if set(_u).intersection(set(_v)):\n",
    "            return LOW_DISTANCE\n",
    "        if not set(ulets).intersection(set(vlets)):\n",
    "            return MEDIUM_DISTANCE\n",
    "        else:\n",
    "            for p in _u:\n",
    "                for q in _v:\n",
    "                    try:\n",
    "                        pval = list(map(int, num_pat.findall(p)))[0]\n",
    "                        qval = list(map(int, num_pat.findall(q)))[0]\n",
    "                        if p[0] == q[0] and (abs(pval - qval) <= 2):\n",
    "                            return SMALL_DISTANCE\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "            return NORMAL_DISTANCE\n",
    "        return MEDIUM_DISTANCE\n",
    "    distances = pdist(df.Cabin.apply(lambda x: x.split() if not isinstance(x, float) else x).values[:, np.newaxis], cabin_distance)\n",
    "    df_ = df.copy()\n",
    "    df_.loc[:, 'cabin_group'] = fcluster(complete(distances), SMALL_DISTANCE, criterion='distance')\n",
    "    df_.cabin_group = df_.groupby('cabin_group')['cabin_group'].transform(lambda x: x if len(x)>1 else pd.Series([-1]*len(x)))\n",
    "    return df_\n",
    "\n",
    "\n",
    "def combine_titles(df):\n",
    "    df_ = df.copy()\n",
    "    df_['title'] = df_['title'].replace(['Mlle'], 'Miss')\n",
    "    df_['title'] = df_['title'].replace(['Ms'], 'Miss')\n",
    "    df_['title'] = df_['title'].replace(['Mme'], 'Mrs')\n",
    "    df_['title'] = df_['title'].replace(['Lady', 'the Countess', 'Capt', 'Col',\n",
    "                                         'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'rare')\n",
    "    return df_\n",
    "\n",
    "\n",
    "def simple_encoder(df):\n",
    "    df_ = df.copy()\n",
    "    sex_mapping = {'male': 0, 'female':1}\n",
    "    embarked_mapping = {'S':0, 'Q':1, 'S':2}\n",
    "    df_.Embarked = df_.Embarked.map(embarked_mapping)\n",
    "    df_.Sex = df_.Sex.map(sex_mapping)\n",
    "    return df_\n",
    "\n",
    "def label_encode(df, **kwargs):\n",
    "    return get_le(df, **kwargs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = (('add_groups', get_ticket_group, {}),\n",
    "                          ('add_family_size', get_family_size, {}),\n",
    "                          ('add_titles', get_titles, {}),\n",
    "                          ('convert_fares', discretize_faries, {'ngroups': 3}),\n",
    "                          ('add_ticket_group', get_ticket_group, {}),\n",
    "                          ('fill_embarked', fill_na_simple, {'colnames': ('Embarked',),\n",
    "                                                             'methods': (lambda x: pd.Series(x).mode()[0],)}),\n",
    "                          ('add_family_name', get_family_name, {}),\n",
    "                          ('add_ages', estimate_age, {}),\n",
    "                          ('convert_ages', discretize_ages, {'ngroups': 5}),\n",
    "                          ('add_cabin_groups', get_cabin_groups, {}),\n",
    "                          ('combine_titles', combine_titles, {}),\n",
    "                          ('sex_encoder', simple_encoder, {}),\n",
    "                          ('drop_columns', drop_columns, {'colnames': ('Survived',\n",
    "                                                                       'PassengerId',\n",
    "                                                                       'SibSp',\n",
    "                                                                       'Parch',\n",
    "                                                                       'Ticket',\n",
    "                                                                       'Fare',\n",
    "                                                                       'family_name',\n",
    "                                                                       'Name',\n",
    "                                                                       'Cabin',\n",
    "                                                                       )}),\n",
    "                          ('get_le', label_encode, {'colnames': ('ticket_group', 'cabin_group',\n",
    "                                                                 'title', 'Embarked')})\n",
    "                         )\n",
    "\n",
    "def process(pipeline, data):\n",
    "    data_ = data.copy()\n",
    "    for name, func, kwargs in pipeline:\n",
    "        print(\"=========== Step: %s ===========\" % name)\n",
    "        data_ = func(data_, **kwargs)\n",
    "        print(\"=\" * 40)\n",
    "    return data_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing steps (feature engeneering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed  = process(preprocessing_pipeline, combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "#clf = SVC(class_weight='balanced', cache_size=8000)\n",
    "\n",
    "\n",
    "X = processed.iloc[:train.shape[0]].values\n",
    "parameters_RF = {'n_estimators': [20,60,80,100,120,200,300,800],\n",
    "              'max_depth': (None, 2, 3, 5, 7, 10, 15),\n",
    "              'criterion': ('gini', 'entropy'),\n",
    "              'max_features': ('auto', 'log2', None),\n",
    "              'oob_score': (True, False)\n",
    "             }\n",
    "\n",
    "\n",
    "parameters_SVC = {'kernel': ['linear', ],\n",
    "                  'C': randint(1, 1000),\n",
    "                  'shrinking': [True, False],\n",
    "                 }\n",
    "\n",
    "parameters_GB = {'n_estimators': randint(20, 300),\n",
    "                 'max_depth': randint(3, 20),\n",
    "                 'subsample': uniform(0.7, 0.3),\n",
    "                 'learning_rate': uniform(0.0001, 0.3),\n",
    "                 'max_features': ('auto', 'log2', None),\n",
    "                 'min_samples_leaf': randint(3, 10),\n",
    "                 'min_samples_split' : randint(2, 10)\n",
    "                }\n",
    "\n",
    "y = train.Survived.values\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    clfcv = RandomizedSearchCV(clf, param_distributions=parameters_GB, n_iter=1000, scoring='f1',\n",
    "                               cv=3, verbose=1, n_jobs=1)\n",
    "    clfcv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clfcv.best_estimator_, X, y, cv=5, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({'PassengerId':test.PassengerId, 'Survived': clfcv.best_estimator_.predict(processed.iloc[train.shape[0]:].values)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "\n",
    "BATCH_SIZE = 80\n",
    "NUM_CLASSES = 2\n",
    "INPUT_SHAPE = processed.shape[1]\n",
    "EPOCHS = 2000\n",
    "\n",
    "# ------------- Building the model ----------------\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(INPUT_SHAPE,), activation='sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(7, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(NUM_CLASSES-1, activation='sigmoid'))\n",
    "# -------------------------------------------------\n",
    "\n",
    "\n",
    "# ---- Building the training and test data --------\n",
    "X = processed.iloc[:train.shape[0]].values\n",
    "Y = train.Survived.values\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42, stratify=Y)\n",
    "\n",
    "#y_train = keras.utils.to_categorical(y_train, 3)\n",
    "#y_test = keras.utils.to_categorical(y_test, 3)\n",
    "# -------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Fitting the model -------------------\n",
    "model.compile(keras.optimizers.Adagrad(), loss=keras.losses.mse, metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=0,\n",
    "          validation_split=0.4)\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
